{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, BatchSampler, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files\n",
    "train_data_df = pd.read_csv('Released_Data/train_data.csv')\n",
    "super_classes_df = pd.read_csv('Released_Data/superclass_mapping.csv')\n",
    "sub_classes_df = pd.read_csv('Released_Data/subclass_mapping.csv')\n",
    "\n",
    "# Rename\n",
    "super_classes_df.rename(columns={'class': 'superclass_name'}, inplace=True)\n",
    "sub_classes_df.rename(columns={'class': 'subclass_name'}, inplace=True)\n",
    "\n",
    "# Merge the class names with the training data\n",
    "train_data_df = train_data_df.merge(super_classes_df, left_on='superclass_index', right_on='index', how='left')\n",
    "train_data_df = train_data_df.merge(sub_classes_df, left_on='subclass_index', right_on='index', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>superclass_index</th>\n",
       "      <th>subclass_index</th>\n",
       "      <th>index_x</th>\n",
       "      <th>superclass_name</th>\n",
       "      <th>index_y</th>\n",
       "      <th>subclass_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>dog</td>\n",
       "      <td>37</td>\n",
       "      <td>Maltese dog, Maltese terrier, Maltese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>bird</td>\n",
       "      <td>42</td>\n",
       "      <td>oystercatcher, oyster catcher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>dog</td>\n",
       "      <td>62</td>\n",
       "      <td>Afghan hound, Afghan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>dog</td>\n",
       "      <td>31</td>\n",
       "      <td>Shih-Tzu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>bird</td>\n",
       "      <td>4</td>\n",
       "      <td>great grey owl, great gray owl, Strix nebulosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image  superclass_index  subclass_index  index_x superclass_name  index_y  \\\n",
       "0  0.jpg                 1              37        1             dog       37   \n",
       "1  1.jpg                 0              42        0            bird       42   \n",
       "2  2.jpg                 1              62        1             dog       62   \n",
       "3  3.jpg                 1              31        1             dog       31   \n",
       "4  4.jpg                 0               4        0            bird        4   \n",
       "\n",
       "                                    subclass_name  \n",
       "0           Maltese dog, Maltese terrier, Maltese  \n",
       "1                   oystercatcher, oyster catcher  \n",
       "2                            Afghan hound, Afghan  \n",
       "3                                        Shih-Tzu  \n",
       "4  great grey owl, great gray owl, Strix nebulosa  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reptile    2388\n",
       "dog        2084\n",
       "bird       1850\n",
       "Name: superclass_name, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df[\"superclass_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset class for multilabel classification\n",
    "class MultiClassImageDataset(Dataset):\n",
    "    def __init__(self, ann_df, super_map_df, sub_map_df, img_dir, transform=None):\n",
    "        self.ann_df = ann_df\n",
    "        self.super_map_df = super_map_df\n",
    "        self.sub_map_df = sub_map_df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ann_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.ann_df['image'][idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        super_idx = self.ann_df['superclass_index'][idx]\n",
    "        super_label = self.super_map_df['class'][super_idx]\n",
    "\n",
    "        sub_idx = self.ann_df['subclass_index'][idx]\n",
    "        sub_label = self.sub_map_df['class'][sub_idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, super_idx, super_label, sub_idx, sub_label\n",
    "\n",
    "class MultiClassImageTestDataset(Dataset):\n",
    "    def __init__(self, super_map_df, sub_map_df, img_dir, transform=None):\n",
    "        self.super_map_df = super_map_df\n",
    "        self.sub_map_df = sub_map_df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): # Count files in img_dir\n",
    "        return len([fname for fname in os.listdir(self.img_dir)])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = str(idx) + '.jpg'\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ann_df = pd.read_csv('Released_Data/train_data.csv')\n",
    "super_map_df = pd.read_csv('Released_Data/superclass_mapping.csv')\n",
    "sub_map_df = pd.read_csv('Released_Data/subclass_mapping.csv')\n",
    "\n",
    "train_img_dir = 'Released_Data/train_shuffle'\n",
    "test_img_dir = 'Released_Data/test_shuffle'\n",
    "\n",
    "image_preprocessing = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0), std=(1)),\n",
    "])\n",
    "\n",
    "# Create train and val split\n",
    "train_dataset = MultiClassImageDataset(train_ann_df, super_map_df, sub_map_df, train_img_dir, transform=image_preprocessing)\n",
    "\n",
    "proportions = [.9, .1]\n",
    "lengths = [int(p * len(train_dataset)) for p in proportions]\n",
    "lengths[-1] = len(train_dataset) - sum(lengths[:-1])\n",
    "#train_dataset, val_dataset = random_split(train_dataset, [0.9, 0.1])\n",
    "# Since I'm using PyTorch 1.1.0, I can't use the above line of code\n",
    "train_dataset, val_dataset = random_split(train_dataset, lengths)\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = MultiClassImageTestDataset(super_map_df, sub_map_df, test_img_dir, transform=image_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "                        nn.Conv2d(3, 32, 3, padding='same'),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(32),\n",
    "                        nn.Conv2d(32, 32, 3, padding='same'),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(32),\n",
    "                        nn.Conv2d(32, 32, 3, padding='same'),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(32),\n",
    "                        nn.MaxPool2d(2, 2)\n",
    "                      )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "                        nn.Conv2d(32, 64, 3, padding='same'),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.Conv2d(64, 64, 3, padding='same'),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.Conv2d(64, 64, 3, padding='same'),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.MaxPool2d(2, 2)\n",
    "                      )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "                        nn.Conv2d(64, 128, 3, padding='same'),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(128),\n",
    "                        nn.Conv2d(128, 128, 3, padding='same'),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(128),\n",
    "                        nn.Conv2d(128, 128, 3, padding='same'),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(128),\n",
    "                        nn.MaxPool2d(2, 2)\n",
    "                      )\n",
    "\n",
    "        self.fc1 = nn.Linear(4*4*128, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3a = nn.Linear(128, 4)\n",
    "        self.fc3b = nn.Linear(128, 88)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        super_out = self.fc3a(x)\n",
    "        sub_out = self.fc3b(x)\n",
    "        return super_out, sub_out\n",
    "\n",
    "resnet18_model = torchvision.models.resnet18()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SCL_fn(features, labels, tau=0.1, mask=None):\n",
    "    \"\"\"\n",
    "    Supervised contrastive loss function.\n",
    "    :param features: tensor of (batch size, feature vector dim)\n",
    "    :param labels: tensor of (batch size)\n",
    "    :param tau: temperature\n",
    "    :return: loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # Supervised contrastive loss (SCL) definition\n",
    "    # https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Contrastive_Learning_Based_Hybrid_Networks_for_Long-Tailed_Image_Classification_CVPR_2021_paper.pdf\n",
    "    # device = torch.device('cpu')\n",
    "    batch_size = features.shape[0]\n",
    "    labels_view = labels.contiguous().view(-1, 1)\n",
    "    positive_pair_mask = torch.eq(labels_view, labels_view.T).float() # Used for identifying positive pairs with anchor\n",
    "    self_contrast_mask = 1 - torch.eye(batch_size, batch_size) # Used for masking out self-contrast entries\n",
    "    \n",
    "    dot_products = (features @ features.T) / tau\n",
    "    exp = torch.exp(dot_products)\n",
    "    numerator_exp = exp * (positive_pair_mask * self_contrast_mask) # for each anchor, mask out self-contrast case and consider only same classes\n",
    "    denominator_exp_sums = torch.sum(exp * self_contrast_mask, dim=1, keepdim=True) # for each anchor, mask out self-contrast case and consider all samples within that batch\n",
    "    \n",
    "    idx = numerator_exp != 0\n",
    "    logs = torch.log(numerator_exp[idx] / denominator_exp_sums)\n",
    "    anchor_sums = torch.sum(logs, dim=1)\n",
    "    loss_per_anchor = (-1 / torch.sum(positive_pair_mask * self_contrast_mask, dim=1)) * anchor_sums\n",
    "    loss = torch.sum(loss_per_anchor)\n",
    "    \n",
    "\n",
    "    \n",
    "    # # device = torch.device('cpu')\n",
    "    # # features = torch.unsqueeze(features, dim=1)\n",
    "\n",
    "    # # if len(features.shape) < 3:\n",
    "    # #     raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
    "    # #                         'at least 3 dimensions are required')\n",
    "    # # if len(features.shape) > 3:\n",
    "    # #     features = features.view(features.shape[0], features.shape[1], -1)\n",
    "\n",
    "    # # batch_size = features.shape[0]\n",
    "    # # if labels is not None and mask is not None:\n",
    "    # #     raise ValueError('Cannot define both `labels` and `mask`')\n",
    "    # # elif labels is None and mask is None:\n",
    "    # #     mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
    "    # # elif labels is not None:\n",
    "    # #     labels = labels.contiguous().view(-1, 1)\n",
    "    # #     if labels.shape[0] != batch_size:\n",
    "    # #         raise ValueError('Num of labels does not match num of features')\n",
    "    # #     mask = torch.eq(labels, labels.T).float().to(device)\n",
    "    # # else:\n",
    "    # #     mask = mask.float().to(device)\n",
    "\n",
    "    # # contrast_count = features.shape[1]\n",
    "    # # contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "    \n",
    "    # # anchor_feature = contrast_feature\n",
    "    # # anchor_count = contrast_count\n",
    "\n",
    "    # # # compute logits\n",
    "    # # anchor_dot_contrast = torch.div(\n",
    "    # #     torch.matmul(anchor_feature, contrast_feature.T),\n",
    "    # #     0.07)\n",
    "    # # # for numerical stability\n",
    "    # # logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "    # # logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "    # # # tile mask\n",
    "    # # mask = mask.repeat(anchor_count, contrast_count)\n",
    "    # # # mask-out self-contrast cases\n",
    "    # # logits_mask = torch.scatter(\n",
    "    # #     torch.ones_like(mask),\n",
    "    # #     1,\n",
    "    # #     torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "    # #     0\n",
    "    # # )\n",
    "    # mask = mask * logits_mask\n",
    "\n",
    "    # # compute log_prob\n",
    "    # exp_logits = torch.exp(logits) * logits_mask\n",
    "    # log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "    # # compute mean of log-likelihood over positive\n",
    "    # mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "\n",
    "    # # loss\n",
    "    # loss = - (0.07 / 0.07) * mean_log_prob_pos\n",
    "    # loss = loss.view(anchor_count, batch_size).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
    "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
    "                 base_temperature=0.07):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "\n",
    "    def forward(self, features, labels=None, mask=None):\n",
    "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, ...].\n",
    "            labels: ground truth of shape [bsz].\n",
    "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
    "                has the same class as sample i. Can be asymmetric.\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        # device = (torch.device('cuda')\n",
    "        #           if features.is_cuda\n",
    "        #           else torch.device('cpu'))\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "        if len(features.shape) < 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
    "                             'at least 3 dimensions are required')\n",
    "        if len(features.shape) > 3:\n",
    "            features = features.view(features.shape[0], features.shape[1], -1)\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        if labels is not None and mask is not None:\n",
    "            raise ValueError('Cannot define both `labels` and `mask`')\n",
    "        elif labels is None and mask is None:\n",
    "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
    "        elif labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        else:\n",
    "            mask = mask.float().to(device)\n",
    "\n",
    "        contrast_count = features.shape[1]\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            self.temperature)\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "\n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        loss = loss.view(anchor_count, batch_size).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTailModel(nn.Module):\n",
    "    # def __init__(self, model_name, target='superclass'):\n",
    "    #     super().__init__()\n",
    "    #     if model_name == 'resnet18':\n",
    "    #         self.resnet_model = torchvision.models.resnet18(pretrained=True)\n",
    "    #         # Replace classification head\n",
    "    #         self.resnet_model.fc = nn.Linear(512, 2000)\n",
    "    #         # non-linear MLP with one hidden layer\n",
    "    #         self.projection_head = nn.Sequential(\n",
    "    #             nn.Linear(2048, 512),\n",
    "    #             nn.ReLU(),\n",
    "    #             nn.Linear(512, 2000),\n",
    "    #             nn.ReLU(),\n",
    "    #         )\n",
    "    #         if target == 'superclass':\n",
    "    #             self.fc = nn.Linear(2000, 4)\n",
    "    #         elif target == 'subclass':\n",
    "    #             self.fc = nn.Linear(2000, 88)\n",
    "    #         else:\n",
    "    #             raise ValueError('target must be superclass or subclass')\n",
    "            \n",
    "    \n",
    "    # def forward(self, x):\n",
    "    #     backbone_output = self.resnet_model(x)\n",
    "    #     backbone_output = F.relu(backbone_output)\n",
    "    #     z = self.projection_head(backbone_output)\n",
    "    #     z_normalized = F.normalize(z, p=2, dim=1)\n",
    "    #     class_logits = self.fc(backbone_output)\n",
    "    #     return z_normalized, class_logits\n",
    "    def __init__(self, model_name, target='superclass'):\n",
    "        super().__init__()\n",
    "        if model_name == 'resnet18':\n",
    "            self.encoder_network = torchvision.models.resnet18(pretrained=True)\n",
    "            # Replace classification head\n",
    "            self.encoder_network.fc = nn.Linear(512, 2048)\n",
    "            # non-linear MLP with one hidden layer\n",
    "            self.projection_head = nn.Sequential(\n",
    "                nn.Linear(2048, 2048),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(2048, 128),\n",
    "            )\n",
    "            if target == 'superclass':\n",
    "                self.fc = nn.Linear(2048, 4)\n",
    "            elif target == 'subclass':\n",
    "                self.fc = nn.Linear(2048, 88)\n",
    "            else:\n",
    "                raise ValueError('target must be superclass or subclass')\n",
    "            \n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoder_output = self.encoder_network(x)\n",
    "        # encoder_output = F.relu(encoder_output)\n",
    "        encoder_output = F.normalize(encoder_output, p=2, dim=1)\n",
    "        z = self.projection_head(encoder_output)\n",
    "        z_normalized = F.normalize(z, p=2, dim=1)\n",
    "        class_logits = self.fc(encoder_output)\n",
    "        return z_normalized, class_logits\n",
    "            \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, criterion1, criterion2, optimizer, train_loader, val_loader, test_loader=None, device='cpu'):\n",
    "        self.model = model\n",
    "        self.contrast_criterion = criterion1\n",
    "        self.class_criterion = criterion2\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        running_contrastive_loss = 0.0\n",
    "        running_classification_loss = 0.0\n",
    "        for i, data in enumerate(self.train_loader):\n",
    "            inputs, super_labels, sub_labels = data[0].to(self.device), data[1].to(self.device), data[3].to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            super_class_feature_vect, super_class_logits = self.model(inputs)\n",
    "            # super_outputs, class_logits = self.model(torch.unsqueeze(inputs))       \n",
    "            # loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
    "            contrastive_loss = self.contrast_criterion(super_class_feature_vect, super_labels)\n",
    "            classification_loss = self.class_criterion(super_class_logits, super_labels)\n",
    "            loss = contrastive_loss + classification_loss\n",
    "            # print(loss)\n",
    "            # print(loss.size())\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            print(f\"Batch: {i}, Total Loss = {loss.item()} Contrastive Loss: {contrastive_loss.item()}, Classification Loss: {classification_loss.item()}\")\n",
    "            running_loss += loss.item()\n",
    "            running_contrastive_loss += contrastive_loss.item()\n",
    "            running_classification_loss += classification_loss.item()\n",
    "\n",
    "        print(f'Training loss: {running_loss/i:.3f}')\n",
    "        print(f'Training contrastive loss: {running_contrastive_loss/i:.3f}')\n",
    "        print(f'Training classification loss: {running_classification_loss/i:.3f}')\n",
    "\n",
    "    def validate_epoch(self):\n",
    "        super_correct = 0\n",
    "        sub_correct = 0\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(self.val_loader):\n",
    "                inputs, super_labels, sub_labels = data[0].to(self.device), data[1].to(self.device), data[3].to(self.device)\n",
    "\n",
    "                _, super_outputs = self.model(inputs)\n",
    "                # loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
    "                # loss = self.criterion(super_outputs, super_labels)\n",
    "                loss = self.contrast_criterion(super_outputs, super_labels)\n",
    "                _, super_predicted = torch.max(super_outputs.data, 1)\n",
    "                # _, sub_predicted = torch.max(sub_outputs.data, 1)\n",
    "\n",
    "                total += super_labels.size(0)\n",
    "                super_correct += (super_predicted == super_labels).sum().item()\n",
    "                # sub_correct += (sub_predicted == sub_labels).sum().item()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        print(f'Validation loss: {running_loss/i:.3f}')\n",
    "        print(f'Validation superclass acc: {100 * super_correct / total:.2f} %')\n",
    "        # print(f'Validation subclass acc: {100 * sub_correct / total:.2f} %')\n",
    "\n",
    "    def test(self, save_to_csv=False, return_predictions=False):\n",
    "        if not self.test_loader:\n",
    "            raise NotImplementedError('test_loader not specified')\n",
    "\n",
    "        # Evaluate on test set, in this simple demo no special care is taken for novel/unseen classes\n",
    "        test_predictions = {'image': [], 'superclass_index': [], 'subclass_index': []}\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(self.test_loader):\n",
    "                inputs, img_name = data[0].to(self.device), data[1]\n",
    "\n",
    "                super_outputs, sub_outputs = self.model(inputs)\n",
    "                _, super_predicted = torch.max(super_outputs.data, 1)\n",
    "                _, sub_predicted = torch.max(sub_outputs.data, 1)\n",
    "\n",
    "                test_predictions['image'].append(img_name[0])\n",
    "                test_predictions['superclass_index'].append(super_predicted.item())\n",
    "                test_predictions['subclass_index'].append(sub_predicted.item())\n",
    "\n",
    "        test_predictions = pd.DataFrame(data=test_predictions)\n",
    "\n",
    "        if save_to_csv:\n",
    "            test_predictions.to_csv('example_test_predictions.csv', index=False)\n",
    "\n",
    "        if return_predictions:\n",
    "            return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(530.1482, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "features = torch.randn(11, 5, requires_grad=True)\n",
    "features = F.normalize(features, p=2, dim=1)\n",
    "labels = torch.Tensor([0,0,0,1,1,0,0,0,2,1,2])\n",
    "loss = SCL_fn(features, labels, tau=0.5)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=1,\n",
    "                         shuffle=False)\n",
    "\n",
    "# Init model and trainer\n",
    "device = 'cpu'\n",
    "model = MultiTailModel(model_name=\"resnet18\", target=\"superclass\").to(device)#CNN().to(device)\n",
    "criterion1 = SCL_fn #nn.CrossEntropyLoss()\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.5)\n",
    "trainer = Trainer(model, criterion1, criterion2, optimizer, train_loader, val_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Batch: 0, Total Loss = 18930.154296875 Contrastive Loss: 18928.76171875, Classification Loss: 1.3921678066253662\n",
      "Batch: 1, Total Loss = 19407.208984375 Contrastive Loss: 19404.9375, Classification Loss: 2.2724287509918213\n",
      "Batch: 2, Total Loss = 18685.212890625 Contrastive Loss: 18675.30078125, Classification Loss: 9.912602424621582\n",
      "Batch: 3, Total Loss = 16993.552734375 Contrastive Loss: 16976.09375, Classification Loss: 17.458293914794922\n",
      "Batch: 4, Total Loss = 17937.759765625 Contrastive Loss: 17924.634765625, Classification Loss: 13.124505996704102\n",
      "Batch: 5, Total Loss = 16954.595703125 Contrastive Loss: 16944.89453125, Classification Loss: 9.701126098632812\n",
      "Batch: 6, Total Loss = 17768.072265625 Contrastive Loss: 17765.205078125, Classification Loss: 2.866980791091919\n",
      "Batch: 7, Total Loss = 17022.15625 Contrastive Loss: 17014.498046875, Classification Loss: 7.657649040222168\n",
      "Batch: 8, Total Loss = 17531.15234375 Contrastive Loss: 17521.89453125, Classification Loss: 9.258363723754883\n",
      "Batch: 9, Total Loss = 17136.396484375 Contrastive Loss: 17126.728515625, Classification Loss: 9.668511390686035\n",
      "Batch: 10, Total Loss = 17735.58203125 Contrastive Loss: 17728.544921875, Classification Loss: 7.037709712982178\n",
      "Batch: 11, Total Loss = 17075.6328125 Contrastive Loss: 17070.087890625, Classification Loss: 5.54403829574585\n",
      "Batch: 12, Total Loss = 17086.0859375 Contrastive Loss: 17080.58984375, Classification Loss: 5.495581150054932\n",
      "Batch: 13, Total Loss = 18909.67578125 Contrastive Loss: 18901.8515625, Classification Loss: 7.823342323303223\n",
      "Batch: 14, Total Loss = 17440.052734375 Contrastive Loss: 17434.361328125, Classification Loss: 5.692144393920898\n",
      "Batch: 15, Total Loss = 17235.0 Contrastive Loss: 17228.248046875, Classification Loss: 6.752018451690674\n",
      "Batch: 16, Total Loss = 16750.4921875 Contrastive Loss: 16744.80078125, Classification Loss: 5.69106388092041\n",
      "Batch: 17, Total Loss = 19543.07421875 Contrastive Loss: 19537.115234375, Classification Loss: 5.959449768066406\n",
      "Batch: 18, Total Loss = 16865.599609375 Contrastive Loss: 16861.109375, Classification Loss: 4.490859031677246\n",
      "Batch: 19, Total Loss = 20040.044921875 Contrastive Loss: 20032.015625, Classification Loss: 8.028547286987305\n",
      "Batch: 20, Total Loss = 16274.384765625 Contrastive Loss: 16271.814453125, Classification Loss: 2.5703916549682617\n",
      "Batch: 21, Total Loss = 17349.404296875 Contrastive Loss: 17346.453125, Classification Loss: 2.950552463531494\n",
      "Batch: 22, Total Loss = 18672.9296875 Contrastive Loss: 18669.912109375, Classification Loss: 3.0177295207977295\n",
      "Batch: 23, Total Loss = 17265.72265625 Contrastive Loss: 17261.53125, Classification Loss: 4.191141128540039\n",
      "Batch: 24, Total Loss = 19601.953125 Contrastive Loss: 19599.484375, Classification Loss: 2.4693961143493652\n",
      "Batch: 25, Total Loss = 16695.3359375 Contrastive Loss: 16691.38671875, Classification Loss: 3.948802947998047\n",
      "Batch: 26, Total Loss = 16457.052734375 Contrastive Loss: 16453.90234375, Classification Loss: 3.1500675678253174\n",
      "Batch: 27, Total Loss = 17404.40625 Contrastive Loss: 17401.39453125, Classification Loss: 3.0111494064331055\n",
      "Batch: 28, Total Loss = 16890.0 Contrastive Loss: 16888.029296875, Classification Loss: 1.971562385559082\n",
      "Batch: 29, Total Loss = 16958.89453125 Contrastive Loss: 16956.802734375, Classification Loss: 2.0915637016296387\n",
      "Batch: 30, Total Loss = 16893.7421875 Contrastive Loss: 16891.271484375, Classification Loss: 2.4698591232299805\n",
      "Batch: 31, Total Loss = 16971.767578125 Contrastive Loss: 16970.490234375, Classification Loss: 1.276563048362732\n",
      "Batch: 32, Total Loss = 16797.828125 Contrastive Loss: 16796.4921875, Classification Loss: 1.3366538286209106\n",
      "Batch: 33, Total Loss = 16670.45703125 Contrastive Loss: 16668.998046875, Classification Loss: 1.459241509437561\n",
      "Batch: 34, Total Loss = 17833.984375 Contrastive Loss: 17831.8984375, Classification Loss: 2.0861270427703857\n",
      "Batch: 35, Total Loss = 17207.60546875 Contrastive Loss: 17206.63671875, Classification Loss: 0.9693400859832764\n",
      "Batch: 36, Total Loss = 16756.912109375 Contrastive Loss: 16755.97265625, Classification Loss: 0.9394054412841797\n",
      "Batch: 37, Total Loss = 17579.833984375 Contrastive Loss: 17578.1171875, Classification Loss: 1.7171828746795654\n",
      "Batch: 38, Total Loss = 16361.392578125 Contrastive Loss: 16360.4228515625, Classification Loss: 0.9701511859893799\n",
      "Batch: 39, Total Loss = 16342.8486328125 Contrastive Loss: 16341.759765625, Classification Loss: 1.088542103767395\n",
      "Batch: 40, Total Loss = 17250.505859375 Contrastive Loss: 17249.189453125, Classification Loss: 1.3159921169281006\n",
      "Batch: 41, Total Loss = 18193.9140625 Contrastive Loss: 18192.400390625, Classification Loss: 1.5127135515213013\n",
      "Batch: 42, Total Loss = 16989.138671875 Contrastive Loss: 16987.892578125, Classification Loss: 1.2469993829727173\n",
      "Batch: 43, Total Loss = 17643.71484375 Contrastive Loss: 17641.63671875, Classification Loss: 2.077298402786255\n",
      "Batch: 44, Total Loss = 16558.291015625 Contrastive Loss: 16556.578125, Classification Loss: 1.7134408950805664\n",
      "Batch: 45, Total Loss = 17492.203125 Contrastive Loss: 17490.728515625, Classification Loss: 1.4741582870483398\n",
      "Batch: 46, Total Loss = 16162.6669921875 Contrastive Loss: 16161.517578125, Classification Loss: 1.149560570716858\n",
      "Batch: 47, Total Loss = 15731.140625 Contrastive Loss: 15730.154296875, Classification Loss: 0.9867367744445801\n",
      "Batch: 48, Total Loss = 16985.498046875 Contrastive Loss: 16984.3828125, Classification Loss: 1.1148730516433716\n",
      "Batch: 49, Total Loss = 16161.6728515625 Contrastive Loss: 16160.45703125, Classification Loss: 1.2157279253005981\n",
      "Batch: 50, Total Loss = 16697.08203125 Contrastive Loss: 16695.859375, Classification Loss: 1.2232732772827148\n",
      "Batch: 51, Total Loss = 17527.88671875 Contrastive Loss: 17525.90234375, Classification Loss: 1.9847277402877808\n",
      "Batch: 52, Total Loss = 16595.083984375 Contrastive Loss: 16594.3984375, Classification Loss: 0.6855785250663757\n",
      "Batch: 53, Total Loss = 15856.2216796875 Contrastive Loss: 15855.0107421875, Classification Loss: 1.210951566696167\n",
      "Batch: 54, Total Loss = 17038.34765625 Contrastive Loss: 17037.171875, Classification Loss: 1.175578236579895\n",
      "Batch: 55, Total Loss = 16073.5849609375 Contrastive Loss: 16072.34375, Classification Loss: 1.2411284446716309\n",
      "Batch: 56, Total Loss = 16740.51171875 Contrastive Loss: 16739.697265625, Classification Loss: 0.8143674731254578\n",
      "Batch: 57, Total Loss = 17722.56640625 Contrastive Loss: 17720.755859375, Classification Loss: 1.8107693195343018\n",
      "Batch: 58, Total Loss = 16308.107421875 Contrastive Loss: 16307.0048828125, Classification Loss: 1.102644443511963\n",
      "Batch: 59, Total Loss = 17632.412109375 Contrastive Loss: 17630.994140625, Classification Loss: 1.4184887409210205\n",
      "Batch: 60, Total Loss = 17670.017578125 Contrastive Loss: 17669.029296875, Classification Loss: 0.988739550113678\n",
      "Batch: 61, Total Loss = 17934.6015625 Contrastive Loss: 17932.58984375, Classification Loss: 2.012162685394287\n",
      "Batch: 62, Total Loss = 18226.236328125 Contrastive Loss: 18224.93359375, Classification Loss: 1.3024550676345825\n",
      "Batch: 63, Total Loss = 16383.0029296875 Contrastive Loss: 16382.021484375, Classification Loss: 0.9819089770317078\n",
      "Batch: 64, Total Loss = 16658.90625 Contrastive Loss: 16657.181640625, Classification Loss: 1.723722219467163\n",
      "Batch: 65, Total Loss = 15993.5908203125 Contrastive Loss: 15992.9140625, Classification Loss: 0.6771318912506104\n",
      "Batch: 66, Total Loss = 16722.291015625 Contrastive Loss: 16720.998046875, Classification Loss: 1.2934277057647705\n",
      "Batch: 67, Total Loss = 17809.791015625 Contrastive Loss: 17808.5078125, Classification Loss: 1.2831851243972778\n",
      "Batch: 68, Total Loss = 15953.9326171875 Contrastive Loss: 15952.9052734375, Classification Loss: 1.0269819498062134\n",
      "Batch: 69, Total Loss = 16169.4345703125 Contrastive Loss: 16168.2998046875, Classification Loss: 1.1346462965011597\n",
      "Batch: 70, Total Loss = 16720.2578125 Contrastive Loss: 16719.447265625, Classification Loss: 0.8103988766670227\n",
      "Batch: 71, Total Loss = 16706.896484375 Contrastive Loss: 16705.849609375, Classification Loss: 1.0475172996520996\n",
      "Batch: 72, Total Loss = 16868.544921875 Contrastive Loss: 16866.9375, Classification Loss: 1.6066572666168213\n",
      "Batch: 73, Total Loss = 16459.28515625 Contrastive Loss: 16458.154296875, Classification Loss: 1.131225824356079\n",
      "Batch: 74, Total Loss = 17126.0390625 Contrastive Loss: 17124.990234375, Classification Loss: 1.0496227741241455\n",
      "Batch: 75, Total Loss = 17143.580078125 Contrastive Loss: 17142.345703125, Classification Loss: 1.2350339889526367\n",
      "Batch: 76, Total Loss = 16752.2265625 Contrastive Loss: 16751.37109375, Classification Loss: 0.8545372486114502\n",
      "Batch: 77, Total Loss = 17059.3671875 Contrastive Loss: 17058.544921875, Classification Loss: 0.822124183177948\n",
      "Batch: 78, Total Loss = 16304.19140625 Contrastive Loss: 16303.302734375, Classification Loss: 0.8890531063079834\n",
      "Batch: 79, Total Loss = 16519.205078125 Contrastive Loss: 16518.19921875, Classification Loss: 1.0061858892440796\n",
      "Batch: 80, Total Loss = 16505.947265625 Contrastive Loss: 16504.958984375, Classification Loss: 0.9879873394966125\n",
      "Batch: 81, Total Loss = 16284.7724609375 Contrastive Loss: 16284.20703125, Classification Loss: 0.565039873123169\n",
      "Batch: 82, Total Loss = 16424.390625 Contrastive Loss: 16423.62890625, Classification Loss: 0.7625394463539124\n",
      "Batch: 83, Total Loss = 16082.2314453125 Contrastive Loss: 16081.421875, Classification Loss: 0.8097610473632812\n",
      "Batch: 84, Total Loss = 16550.080078125 Contrastive Loss: 16549.623046875, Classification Loss: 0.4563198685646057\n",
      "Batch: 85, Total Loss = 16950.49609375 Contrastive Loss: 16949.5703125, Classification Loss: 0.9252951741218567\n",
      "Batch: 86, Total Loss = 17044.201171875 Contrastive Loss: 17042.98828125, Classification Loss: 1.2132723331451416\n",
      "Batch: 87, Total Loss = 16710.998046875 Contrastive Loss: 16710.076171875, Classification Loss: 0.9213043451309204\n",
      "Batch: 88, Total Loss = 12770.701171875 Contrastive Loss: 12769.8779296875, Classification Loss: 0.8236915469169617\n",
      "Training loss: 17271.588\n",
      "Training contrastive loss: 17268.789\n",
      "Training classification loss: 2.799\n",
      "Validation loss: nan\n",
      "Validation superclass acc: 33.02 %\n",
      "\n",
      "Epoch 2\n",
      "Batch: 0, Total Loss = 17825.326171875 Contrastive Loss: 17824.640625, Classification Loss: 0.6853131055831909\n",
      "Batch: 1, Total Loss = 16745.4609375 Contrastive Loss: 16744.2578125, Classification Loss: 1.2039415836334229\n",
      "Batch: 2, Total Loss = 15994.7705078125 Contrastive Loss: 15993.7822265625, Classification Loss: 0.9881590008735657\n",
      "Batch: 3, Total Loss = 16906.703125 Contrastive Loss: 16905.75390625, Classification Loss: 0.949943482875824\n",
      "Batch: 4, Total Loss = 18465.873046875 Contrastive Loss: 18465.033203125, Classification Loss: 0.8395364284515381\n",
      "Batch: 5, Total Loss = 16377.052734375 Contrastive Loss: 16376.1572265625, Classification Loss: 0.8958112597465515\n",
      "Batch: 6, Total Loss = 17907.103515625 Contrastive Loss: 17906.423828125, Classification Loss: 0.6799812912940979\n",
      "Batch: 7, Total Loss = 17351.951171875 Contrastive Loss: 17351.0859375, Classification Loss: 0.8645245432853699\n",
      "Batch: 8, Total Loss = 15447.54296875 Contrastive Loss: 15445.7470703125, Classification Loss: 1.79632568359375\n",
      "Batch: 9, Total Loss = 15584.388671875 Contrastive Loss: 15583.4091796875, Classification Loss: 0.9797247052192688\n",
      "Batch: 10, Total Loss = 17045.962890625 Contrastive Loss: 17044.822265625, Classification Loss: 1.1406352519989014\n",
      "Batch: 11, Total Loss = 15884.556640625 Contrastive Loss: 15883.255859375, Classification Loss: 1.3010333776474\n",
      "Batch: 12, Total Loss = 16250.2265625 Contrastive Loss: 16248.380859375, Classification Loss: 1.8460171222686768\n",
      "Batch: 13, Total Loss = 16550.576171875 Contrastive Loss: 16549.388671875, Classification Loss: 1.1865346431732178\n",
      "Batch: 14, Total Loss = 16529.8828125 Contrastive Loss: 16528.326171875, Classification Loss: 1.5567799806594849\n",
      "Batch: 15, Total Loss = 17910.3671875 Contrastive Loss: 17909.685546875, Classification Loss: 0.6810633540153503\n",
      "Batch: 16, Total Loss = 16118.9150390625 Contrastive Loss: 16117.4931640625, Classification Loss: 1.421481728553772\n",
      "Batch: 17, Total Loss = 16321.4462890625 Contrastive Loss: 16319.8408203125, Classification Loss: 1.6058019399642944\n",
      "Batch: 18, Total Loss = 16611.73828125 Contrastive Loss: 16610.1953125, Classification Loss: 1.5430195331573486\n",
      "Batch: 19, Total Loss = 16205.9453125 Contrastive Loss: 16205.1533203125, Classification Loss: 0.7924094200134277\n",
      "Batch: 20, Total Loss = 17308.42578125 Contrastive Loss: 17307.783203125, Classification Loss: 0.6424572467803955\n",
      "Batch: 21, Total Loss = 15727.525390625 Contrastive Loss: 15726.4140625, Classification Loss: 1.110843300819397\n",
      "Batch: 22, Total Loss = 16555.478515625 Contrastive Loss: 16554.3359375, Classification Loss: 1.1428366899490356\n",
      "Batch: 23, Total Loss = 17792.572265625 Contrastive Loss: 17790.5078125, Classification Loss: 2.064643383026123\n",
      "Batch: 24, Total Loss = 15716.470703125 Contrastive Loss: 15715.9755859375, Classification Loss: 0.495350182056427\n",
      "Batch: 25, Total Loss = 16225.5322265625 Contrastive Loss: 16224.5263671875, Classification Loss: 1.0056918859481812\n",
      "Batch: 26, Total Loss = 16407.22265625 Contrastive Loss: 16406.234375, Classification Loss: 0.9891555905342102\n",
      "Batch: 27, Total Loss = 17604.146484375 Contrastive Loss: 17602.916015625, Classification Loss: 1.2308701276779175\n",
      "Batch: 28, Total Loss = 18156.033203125 Contrastive Loss: 18154.955078125, Classification Loss: 1.0773193836212158\n",
      "Batch: 29, Total Loss = 16044.330078125 Contrastive Loss: 16042.7890625, Classification Loss: 1.5412216186523438\n",
      "Batch: 30, Total Loss = 16684.54296875 Contrastive Loss: 16683.833984375, Classification Loss: 0.7084689736366272\n",
      "Batch: 31, Total Loss = 15495.7890625 Contrastive Loss: 15495.0263671875, Classification Loss: 0.7626574635505676\n",
      "Batch: 32, Total Loss = 16688.6796875 Contrastive Loss: 16688.0625, Classification Loss: 0.6166883707046509\n",
      "Batch: 33, Total Loss = 15962.208984375 Contrastive Loss: 15961.166015625, Classification Loss: 1.043292760848999\n",
      "Batch: 34, Total Loss = 17632.76171875 Contrastive Loss: 17631.4765625, Classification Loss: 1.2843878269195557\n",
      "Batch: 35, Total Loss = 16377.6318359375 Contrastive Loss: 16376.8369140625, Classification Loss: 0.7949666976928711\n",
      "Batch: 36, Total Loss = 18009.5859375 Contrastive Loss: 18007.38671875, Classification Loss: 2.198347330093384\n",
      "Batch: 37, Total Loss = 15958.4306640625 Contrastive Loss: 15957.6650390625, Classification Loss: 0.7652618885040283\n",
      "Batch: 38, Total Loss = 16637.576171875 Contrastive Loss: 16636.68359375, Classification Loss: 0.8928388953208923\n",
      "Batch: 39, Total Loss = 16934.77734375 Contrastive Loss: 16933.275390625, Classification Loss: 1.5017880201339722\n",
      "Batch: 40, Total Loss = 15656.7568359375 Contrastive Loss: 15655.7421875, Classification Loss: 1.0147968530654907\n",
      "Batch: 41, Total Loss = 16093.7021484375 Contrastive Loss: 16093.025390625, Classification Loss: 0.6765783429145813\n",
      "Batch: 42, Total Loss = 15557.42578125 Contrastive Loss: 15556.669921875, Classification Loss: 0.7559895515441895\n",
      "Batch: 43, Total Loss = 16150.259765625 Contrastive Loss: 16149.1259765625, Classification Loss: 1.1341168880462646\n",
      "Batch: 44, Total Loss = 16814.955078125 Contrastive Loss: 16813.96875, Classification Loss: 0.9862198233604431\n",
      "Batch: 45, Total Loss = 16250.4501953125 Contrastive Loss: 16249.61328125, Classification Loss: 0.8368223309516907\n",
      "Batch: 46, Total Loss = 16905.763671875 Contrastive Loss: 16904.58203125, Classification Loss: 1.1809309720993042\n",
      "Batch: 47, Total Loss = 16621.08984375 Contrastive Loss: 16620.27734375, Classification Loss: 0.812648594379425\n",
      "Batch: 48, Total Loss = 17509.373046875 Contrastive Loss: 17508.29296875, Classification Loss: 1.0805079936981201\n",
      "Batch: 49, Total Loss = 16185.92578125 Contrastive Loss: 16184.53515625, Classification Loss: 1.3902616500854492\n",
      "Batch: 50, Total Loss = 16167.642578125 Contrastive Loss: 16167.0517578125, Classification Loss: 0.5904208421707153\n",
      "Batch: 51, Total Loss = 15989.4765625 Contrastive Loss: 15988.203125, Classification Loss: 1.2729616165161133\n",
      "Batch: 52, Total Loss = 16068.6640625 Contrastive Loss: 16067.93359375, Classification Loss: 0.7303273677825928\n",
      "Batch: 53, Total Loss = 15881.0888671875 Contrastive Loss: 15880.421875, Classification Loss: 0.6665064692497253\n",
      "Batch: 54, Total Loss = 16262.287109375 Contrastive Loss: 16261.3974609375, Classification Loss: 0.889279305934906\n",
      "Batch: 55, Total Loss = 16052.5263671875 Contrastive Loss: 16051.9150390625, Classification Loss: 0.6114771366119385\n",
      "Batch: 56, Total Loss = 15390.154296875 Contrastive Loss: 15389.546875, Classification Loss: 0.6072709560394287\n",
      "Batch: 57, Total Loss = 16515.6796875 Contrastive Loss: 16514.931640625, Classification Loss: 0.748633861541748\n",
      "Batch: 58, Total Loss = 17872.6875 Contrastive Loss: 17871.69140625, Classification Loss: 0.9961653351783752\n",
      "Batch: 59, Total Loss = 16592.7734375 Contrastive Loss: 16592.1015625, Classification Loss: 0.6709985136985779\n",
      "Batch: 60, Total Loss = 16415.974609375 Contrastive Loss: 16415.044921875, Classification Loss: 0.9287163019180298\n",
      "Batch: 61, Total Loss = 15805.806640625 Contrastive Loss: 15805.1044921875, Classification Loss: 0.7020313739776611\n",
      "Batch: 62, Total Loss = 16656.6328125 Contrastive Loss: 16655.677734375, Classification Loss: 0.9542089104652405\n",
      "Batch: 63, Total Loss = 16008.7158203125 Contrastive Loss: 16008.169921875, Classification Loss: 0.5457146763801575\n",
      "Batch: 64, Total Loss = 16732.970703125 Contrastive Loss: 16732.236328125, Classification Loss: 0.7343453168869019\n",
      "Batch: 65, Total Loss = 17453.517578125 Contrastive Loss: 17452.345703125, Classification Loss: 1.1720507144927979\n",
      "Batch: 66, Total Loss = 15454.9453125 Contrastive Loss: 15454.3837890625, Classification Loss: 0.5616062879562378\n",
      "Batch: 67, Total Loss = 15971.912109375 Contrastive Loss: 15971.3046875, Classification Loss: 0.607144296169281\n",
      "Batch: 68, Total Loss = 16306.453125 Contrastive Loss: 16305.7265625, Classification Loss: 0.726270854473114\n",
      "Batch: 69, Total Loss = 16941.376953125 Contrastive Loss: 16940.232421875, Classification Loss: 1.1446396112442017\n",
      "Batch: 70, Total Loss = 16534.943359375 Contrastive Loss: 16533.974609375, Classification Loss: 0.9677941799163818\n",
      "Batch: 71, Total Loss = 15756.513671875 Contrastive Loss: 15755.515625, Classification Loss: 0.998380184173584\n",
      "Batch: 72, Total Loss = 16160.4228515625 Contrastive Loss: 16159.359375, Classification Loss: 1.0638879537582397\n",
      "Batch: 73, Total Loss = 15614.9609375 Contrastive Loss: 15614.412109375, Classification Loss: 0.5490180253982544\n",
      "Batch: 74, Total Loss = 17228.134765625 Contrastive Loss: 17227.15625, Classification Loss: 0.9779168367385864\n",
      "Batch: 75, Total Loss = 16694.669921875 Contrastive Loss: 16692.73828125, Classification Loss: 1.9325264692306519\n",
      "Batch: 76, Total Loss = 15856.791015625 Contrastive Loss: 15855.6826171875, Classification Loss: 1.1086660623550415\n",
      "Batch: 77, Total Loss = 15888.2138671875 Contrastive Loss: 15887.2900390625, Classification Loss: 0.9240525364875793\n",
      "Batch: 78, Total Loss = 16320.53125 Contrastive Loss: 16319.265625, Classification Loss: 1.265543818473816\n",
      "Batch: 79, Total Loss = 17275.775390625 Contrastive Loss: 17273.728515625, Classification Loss: 2.0468034744262695\n",
      "Batch: 80, Total Loss = 15854.322265625 Contrastive Loss: 15853.4375, Classification Loss: 0.8843469619750977\n",
      "Batch: 81, Total Loss = 16379.7744140625 Contrastive Loss: 16378.5068359375, Classification Loss: 1.2674270868301392\n",
      "Batch: 82, Total Loss = 15684.8583984375 Contrastive Loss: 15684.005859375, Classification Loss: 0.8522391319274902\n",
      "Batch: 83, Total Loss = 16531.88671875 Contrastive Loss: 16530.9140625, Classification Loss: 0.9731222987174988\n",
      "Batch: 84, Total Loss = 16012.5703125 Contrastive Loss: 16011.083984375, Classification Loss: 1.4861459732055664\n",
      "Batch: 85, Total Loss = 17338.66015625 Contrastive Loss: 17337.974609375, Classification Loss: 0.6848974823951721\n",
      "Batch: 86, Total Loss = 16562.912109375 Contrastive Loss: 16561.421875, Classification Loss: 1.4898223876953125\n",
      "Batch: 87, Total Loss = 16068.1328125 Contrastive Loss: 16067.333984375, Classification Loss: 0.7990075945854187\n",
      "Batch: 88, Total Loss = 12615.37109375 Contrastive Loss: 12614.5908203125, Classification Loss: 0.7799471020698547\n",
      "Training loss: 16643.431\n",
      "Training contrastive loss: 16642.390\n",
      "Training classification loss: 1.041\n",
      "Validation loss: nan\n",
      "Validation superclass acc: 65.40 %\n",
      "\n",
      "Epoch 3\n",
      "Batch: 0, Total Loss = 16046.01953125 Contrastive Loss: 16044.9501953125, Classification Loss: 1.0690724849700928\n",
      "Batch: 1, Total Loss = 16156.552734375 Contrastive Loss: 16155.513671875, Classification Loss: 1.039023756980896\n",
      "Batch: 2, Total Loss = 15602.5400390625 Contrastive Loss: 15601.8642578125, Classification Loss: 0.675752580165863\n",
      "Batch: 3, Total Loss = 15387.2626953125 Contrastive Loss: 15386.70703125, Classification Loss: 0.5561134815216064\n",
      "Batch: 4, Total Loss = 16012.0 Contrastive Loss: 16011.6171875, Classification Loss: 0.3831965923309326\n",
      "Batch: 5, Total Loss = 16586.9921875 Contrastive Loss: 16586.34765625, Classification Loss: 0.6435766816139221\n",
      "Batch: 6, Total Loss = 16053.0048828125 Contrastive Loss: 16052.3291015625, Classification Loss: 0.6756811141967773\n",
      "Batch: 7, Total Loss = 16822.306640625 Contrastive Loss: 16821.763671875, Classification Loss: 0.5426071882247925\n",
      "Batch: 8, Total Loss = 17318.23046875 Contrastive Loss: 17317.478515625, Classification Loss: 0.7515841722488403\n",
      "Batch: 9, Total Loss = 15695.2666015625 Contrastive Loss: 15694.501953125, Classification Loss: 0.7651081085205078\n",
      "Batch: 10, Total Loss = 15506.6904296875 Contrastive Loss: 15505.8232421875, Classification Loss: 0.8670905232429504\n",
      "Batch: 11, Total Loss = 16412.400390625 Contrastive Loss: 16411.654296875, Classification Loss: 0.745526134967804\n",
      "Batch: 12, Total Loss = 16098.890625 Contrastive Loss: 16097.966796875, Classification Loss: 0.9240512847900391\n",
      "Batch: 13, Total Loss = 16266.1298828125 Contrastive Loss: 16265.12109375, Classification Loss: 1.0090394020080566\n",
      "Batch: 14, Total Loss = 16556.783203125 Contrastive Loss: 16555.041015625, Classification Loss: 1.742803692817688\n",
      "Batch: 15, Total Loss = 17026.751953125 Contrastive Loss: 17026.064453125, Classification Loss: 0.6877166032791138\n",
      "Batch: 16, Total Loss = 16981.11328125 Contrastive Loss: 16979.802734375, Classification Loss: 1.3106948137283325\n",
      "Batch: 17, Total Loss = 15889.015625 Contrastive Loss: 15888.263671875, Classification Loss: 0.7517983317375183\n",
      "Batch: 18, Total Loss = 15377.7998046875 Contrastive Loss: 15377.2685546875, Classification Loss: 0.5311418175697327\n",
      "Batch: 19, Total Loss = 15351.3671875 Contrastive Loss: 15350.732421875, Classification Loss: 0.6343106031417847\n",
      "Batch: 20, Total Loss = 16734.9140625 Contrastive Loss: 16734.212890625, Classification Loss: 0.701600968837738\n",
      "Batch: 21, Total Loss = 17431.861328125 Contrastive Loss: 17431.240234375, Classification Loss: 0.6213212609291077\n",
      "Batch: 22, Total Loss = 18207.30859375 Contrastive Loss: 18204.82421875, Classification Loss: 2.4836857318878174\n",
      "Batch: 23, Total Loss = 16387.916015625 Contrastive Loss: 16387.203125, Classification Loss: 0.7123286724090576\n",
      "Batch: 24, Total Loss = 16223.4091796875 Contrastive Loss: 16222.9072265625, Classification Loss: 0.5018558502197266\n",
      "Batch: 25, Total Loss = 15838.1640625 Contrastive Loss: 15837.623046875, Classification Loss: 0.5411351323127747\n",
      "Batch: 26, Total Loss = 15356.4287109375 Contrastive Loss: 15355.7294921875, Classification Loss: 0.699409544467926\n",
      "Batch: 27, Total Loss = 15529.033203125 Contrastive Loss: 15528.4375, Classification Loss: 0.5952452421188354\n",
      "Batch: 28, Total Loss = 15177.986328125 Contrastive Loss: 15177.5009765625, Classification Loss: 0.48559486865997314\n",
      "Batch: 29, Total Loss = 15321.775390625 Contrastive Loss: 15321.1875, Classification Loss: 0.588232696056366\n",
      "Batch: 30, Total Loss = 17114.421875 Contrastive Loss: 17114.091796875, Classification Loss: 0.3306853771209717\n",
      "Batch: 31, Total Loss = 17023.615234375 Contrastive Loss: 17022.56640625, Classification Loss: 1.0493676662445068\n",
      "Batch: 32, Total Loss = 16302.921875 Contrastive Loss: 16302.3916015625, Classification Loss: 0.5305450558662415\n",
      "Batch: 33, Total Loss = 15680.3935546875 Contrastive Loss: 15679.77734375, Classification Loss: 0.6158657670021057\n",
      "Batch: 34, Total Loss = 16582.48046875 Contrastive Loss: 16581.56640625, Classification Loss: 0.913203239440918\n",
      "Batch: 35, Total Loss = 16223.4052734375 Contrastive Loss: 16223.01953125, Classification Loss: 0.3858442008495331\n",
      "Batch: 36, Total Loss = 16052.8330078125 Contrastive Loss: 16052.3740234375, Classification Loss: 0.4586518406867981\n",
      "Batch: 37, Total Loss = 15358.505859375 Contrastive Loss: 15358.0625, Classification Loss: 0.44297075271606445\n",
      "Batch: 38, Total Loss = 16391.607421875 Contrastive Loss: 16391.0703125, Classification Loss: 0.5369322299957275\n",
      "Batch: 39, Total Loss = 16234.24609375 Contrastive Loss: 16233.68359375, Classification Loss: 0.562329113483429\n",
      "Batch: 40, Total Loss = 16561.052734375 Contrastive Loss: 16560.361328125, Classification Loss: 0.6916983127593994\n",
      "Batch: 41, Total Loss = 15683.189453125 Contrastive Loss: 15682.60546875, Classification Loss: 0.5841917991638184\n",
      "Batch: 42, Total Loss = 15267.6533203125 Contrastive Loss: 15267.1025390625, Classification Loss: 0.5509474873542786\n",
      "Batch: 43, Total Loss = 16398.66796875 Contrastive Loss: 16398.080078125, Classification Loss: 0.5883851647377014\n",
      "Batch: 44, Total Loss = 16215.8330078125 Contrastive Loss: 16214.90234375, Classification Loss: 0.9302353858947754\n",
      "Batch: 45, Total Loss = 17485.4453125 Contrastive Loss: 17484.40234375, Classification Loss: 1.0435247421264648\n",
      "Batch: 46, Total Loss = 16638.63671875 Contrastive Loss: 16637.791015625, Classification Loss: 0.8458846807479858\n",
      "Batch: 47, Total Loss = 16076.810546875 Contrastive Loss: 16076.1474609375, Classification Loss: 0.6634435653686523\n",
      "Batch: 48, Total Loss = 15658.7705078125 Contrastive Loss: 15658.333984375, Classification Loss: 0.4367511570453644\n",
      "Batch: 49, Total Loss = 16944.326171875 Contrastive Loss: 16943.462890625, Classification Loss: 0.8638107180595398\n",
      "Batch: 50, Total Loss = 15656.6025390625 Contrastive Loss: 15655.6875, Classification Loss: 0.9147198796272278\n",
      "Batch: 51, Total Loss = 17772.38671875 Contrastive Loss: 17771.3984375, Classification Loss: 0.9881432056427002\n",
      "Batch: 52, Total Loss = 17821.73046875 Contrastive Loss: 17820.56640625, Classification Loss: 1.1639431715011597\n",
      "Batch: 53, Total Loss = 15813.3896484375 Contrastive Loss: 15812.591796875, Classification Loss: 0.7979462742805481\n",
      "Batch: 54, Total Loss = 15788.1904296875 Contrastive Loss: 15787.59765625, Classification Loss: 0.5927574038505554\n",
      "Batch: 55, Total Loss = 16084.5087890625 Contrastive Loss: 16084.015625, Classification Loss: 0.49274328351020813\n",
      "Batch: 56, Total Loss = 16788.533203125 Contrastive Loss: 16788.0234375, Classification Loss: 0.5104398131370544\n",
      "Batch: 57, Total Loss = 17404.47265625 Contrastive Loss: 17403.76171875, Classification Loss: 0.7110583186149597\n",
      "Batch: 58, Total Loss = 16123.1181640625 Contrastive Loss: 16122.7119140625, Classification Loss: 0.4064510464668274\n",
      "Batch: 59, Total Loss = 16886.724609375 Contrastive Loss: 16885.8828125, Classification Loss: 0.8421303033828735\n",
      "Batch: 60, Total Loss = 17126.4296875 Contrastive Loss: 17125.78515625, Classification Loss: 0.644882082939148\n",
      "Batch: 61, Total Loss = 16171.9853515625 Contrastive Loss: 16170.7314453125, Classification Loss: 1.2543389797210693\n",
      "Batch: 62, Total Loss = 17166.81640625 Contrastive Loss: 17166.115234375, Classification Loss: 0.7009019255638123\n",
      "Batch: 63, Total Loss = 16053.7314453125 Contrastive Loss: 16052.8818359375, Classification Loss: 0.8493499159812927\n",
      "Batch: 64, Total Loss = 15489.3271484375 Contrastive Loss: 15488.8671875, Classification Loss: 0.45988744497299194\n",
      "Batch: 65, Total Loss = 15749.08203125 Contrastive Loss: 15748.4365234375, Classification Loss: 0.6458165645599365\n",
      "Batch: 66, Total Loss = 15465.107421875 Contrastive Loss: 15464.6171875, Classification Loss: 0.48991063237190247\n",
      "Batch: 67, Total Loss = 16616.5390625 Contrastive Loss: 16615.48828125, Classification Loss: 1.051573634147644\n",
      "Batch: 68, Total Loss = 16635.74609375 Contrastive Loss: 16634.9140625, Classification Loss: 0.8317765593528748\n",
      "Batch: 69, Total Loss = 15351.0234375 Contrastive Loss: 15350.380859375, Classification Loss: 0.6423016786575317\n",
      "Batch: 70, Total Loss = 15262.6953125 Contrastive Loss: 15261.982421875, Classification Loss: 0.7133331894874573\n",
      "Batch: 71, Total Loss = 15691.0703125 Contrastive Loss: 15690.66015625, Classification Loss: 0.41037118434906006\n",
      "Batch: 72, Total Loss = 16507.302734375 Contrastive Loss: 16506.759765625, Classification Loss: 0.5435393452644348\n",
      "Batch: 73, Total Loss = 17661.861328125 Contrastive Loss: 17661.146484375, Classification Loss: 0.7145375609397888\n",
      "Batch: 74, Total Loss = 15696.5703125 Contrastive Loss: 15695.5673828125, Classification Loss: 1.0030162334442139\n",
      "Batch: 75, Total Loss = 17222.177734375 Contrastive Loss: 17221.60546875, Classification Loss: 0.5723613500595093\n",
      "Batch: 76, Total Loss = 15513.8349609375 Contrastive Loss: 15513.1025390625, Classification Loss: 0.7319375276565552\n",
      "Batch: 77, Total Loss = 17702.451171875 Contrastive Loss: 17701.650390625, Classification Loss: 0.8017198443412781\n",
      "Batch: 78, Total Loss = 16086.5986328125 Contrastive Loss: 16085.7666015625, Classification Loss: 0.8321177363395691\n",
      "Batch: 79, Total Loss = 15509.048828125 Contrastive Loss: 15508.6484375, Classification Loss: 0.40062981843948364\n",
      "Batch: 80, Total Loss = 15861.5458984375 Contrastive Loss: 15860.998046875, Classification Loss: 0.548100471496582\n",
      "Batch: 81, Total Loss = 15675.236328125 Contrastive Loss: 15674.4228515625, Classification Loss: 0.8131030201911926\n",
      "Batch: 82, Total Loss = 16577.57421875 Contrastive Loss: 16577.0546875, Classification Loss: 0.5189290642738342\n",
      "Batch: 83, Total Loss = 15838.724609375 Contrastive Loss: 15837.314453125, Classification Loss: 1.4101413488388062\n",
      "Batch: 84, Total Loss = 15853.419921875 Contrastive Loss: 15852.2685546875, Classification Loss: 1.1517504453659058\n",
      "Batch: 85, Total Loss = 15129.068359375 Contrastive Loss: 15128.3837890625, Classification Loss: 0.6849746108055115\n",
      "Batch: 86, Total Loss = 17394.828125 Contrastive Loss: 17393.4296875, Classification Loss: 1.39823579788208\n",
      "Batch: 87, Total Loss = 17559.25390625 Contrastive Loss: 17558.6875, Classification Loss: 0.5670536756515503\n",
      "Batch: 88, Total Loss = 11319.052734375 Contrastive Loss: 11318.109375, Classification Loss: 0.9429916739463806\n",
      "Training loss: 16400.892\n",
      "Training contrastive loss: 16400.131\n",
      "Training classification loss: 0.761\n",
      "Validation loss: nan\n",
      "Validation superclass acc: 67.61 %\n",
      "\n",
      "Epoch 4\n",
      "Batch: 0, Total Loss = 14848.8974609375 Contrastive Loss: 14848.234375, Classification Loss: 0.6632557511329651\n",
      "Batch: 1, Total Loss = 17528.193359375 Contrastive Loss: 17526.84765625, Classification Loss: 1.345592975616455\n",
      "Batch: 2, Total Loss = 15567.0419921875 Contrastive Loss: 15565.7529296875, Classification Loss: 1.2894526720046997\n",
      "Batch: 3, Total Loss = 15970.0185546875 Contrastive Loss: 15968.576171875, Classification Loss: 1.4424419403076172\n",
      "Batch: 4, Total Loss = 15502.806640625 Contrastive Loss: 15501.7021484375, Classification Loss: 1.1045469045639038\n",
      "Batch: 5, Total Loss = 16730.5859375 Contrastive Loss: 16729.33203125, Classification Loss: 1.2531177997589111\n",
      "Batch: 6, Total Loss = 15463.0166015625 Contrastive Loss: 15461.5712890625, Classification Loss: 1.4451944828033447\n",
      "Batch: 7, Total Loss = 15301.224609375 Contrastive Loss: 15300.76171875, Classification Loss: 0.4630900025367737\n",
      "Batch: 8, Total Loss = 15185.1337890625 Contrastive Loss: 15184.3955078125, Classification Loss: 0.7384260296821594\n",
      "Batch: 9, Total Loss = 16335.6513671875 Contrastive Loss: 16334.326171875, Classification Loss: 1.3250861167907715\n",
      "Batch: 10, Total Loss = 16579.384765625 Contrastive Loss: 16578.078125, Classification Loss: 1.3061017990112305\n",
      "Batch: 11, Total Loss = 15273.4013671875 Contrastive Loss: 15271.919921875, Classification Loss: 1.4816771745681763\n",
      "Batch: 12, Total Loss = 14945.353515625 Contrastive Loss: 14944.166015625, Classification Loss: 1.1874796152114868\n",
      "Batch: 13, Total Loss = 16317.30078125 Contrastive Loss: 16315.5380859375, Classification Loss: 1.7624961137771606\n",
      "Batch: 14, Total Loss = 16133.2373046875 Contrastive Loss: 16132.09765625, Classification Loss: 1.139647364616394\n",
      "Batch: 15, Total Loss = 16326.078125 Contrastive Loss: 16324.60546875, Classification Loss: 1.472226619720459\n",
      "Batch: 16, Total Loss = 16558.82421875 Contrastive Loss: 16557.396484375, Classification Loss: 1.4271199703216553\n",
      "Batch: 17, Total Loss = 16828.30078125 Contrastive Loss: 16826.978515625, Classification Loss: 1.322694182395935\n",
      "Batch: 18, Total Loss = 16363.900390625 Contrastive Loss: 16362.583984375, Classification Loss: 1.3161327838897705\n",
      "Batch: 19, Total Loss = 15578.52734375 Contrastive Loss: 15577.44140625, Classification Loss: 1.0855382680892944\n",
      "Batch: 20, Total Loss = 16439.2421875 Contrastive Loss: 16438.21484375, Classification Loss: 1.0273218154907227\n",
      "Batch: 21, Total Loss = 16096.0166015625 Contrastive Loss: 16095.09375, Classification Loss: 0.9230262637138367\n",
      "Batch: 22, Total Loss = 16058.705078125 Contrastive Loss: 16057.25390625, Classification Loss: 1.4513766765594482\n",
      "Batch: 23, Total Loss = 15000.7509765625 Contrastive Loss: 15000.1669921875, Classification Loss: 0.5840235352516174\n",
      "Batch: 24, Total Loss = 16406.837890625 Contrastive Loss: 16406.27734375, Classification Loss: 0.5605815649032593\n",
      "Batch: 25, Total Loss = 15447.5595703125 Contrastive Loss: 15446.20703125, Classification Loss: 1.3524868488311768\n",
      "Batch: 26, Total Loss = 16054.5986328125 Contrastive Loss: 16053.9267578125, Classification Loss: 0.671504557132721\n",
      "Batch: 27, Total Loss = 15584.984375 Contrastive Loss: 15583.42578125, Classification Loss: 1.558417558670044\n",
      "Batch: 28, Total Loss = 17290.27734375 Contrastive Loss: 17287.974609375, Classification Loss: 2.302928924560547\n",
      "Batch: 29, Total Loss = 15223.63671875 Contrastive Loss: 15222.513671875, Classification Loss: 1.1231783628463745\n",
      "Batch: 30, Total Loss = 16797.544921875 Contrastive Loss: 16795.46875, Classification Loss: 2.076972484588623\n",
      "Batch: 31, Total Loss = 16298.6279296875 Contrastive Loss: 16296.337890625, Classification Loss: 2.2900192737579346\n",
      "Batch: 32, Total Loss = 15858.7548828125 Contrastive Loss: 15856.533203125, Classification Loss: 2.2212159633636475\n",
      "Batch: 33, Total Loss = 15285.3154296875 Contrastive Loss: 15284.5654296875, Classification Loss: 0.7496947050094604\n",
      "Batch: 34, Total Loss = 17101.02734375 Contrastive Loss: 17099.7734375, Classification Loss: 1.2535231113433838\n",
      "Batch: 35, Total Loss = 16896.5 Contrastive Loss: 16894.634765625, Classification Loss: 1.865709662437439\n",
      "Batch: 36, Total Loss = 17744.11328125 Contrastive Loss: 17743.16015625, Classification Loss: 0.9532667994499207\n",
      "Batch: 37, Total Loss = 15667.9453125 Contrastive Loss: 15666.8662109375, Classification Loss: 1.078715443611145\n",
      "Batch: 38, Total Loss = 15587.619140625 Contrastive Loss: 15587.16015625, Classification Loss: 0.45911726355552673\n",
      "Batch: 39, Total Loss = 16537.37890625 Contrastive Loss: 16536.318359375, Classification Loss: 1.06120765209198\n",
      "Batch: 40, Total Loss = 15272.6962890625 Contrastive Loss: 15271.712890625, Classification Loss: 0.9836369156837463\n",
      "Batch: 41, Total Loss = 15679.125 Contrastive Loss: 15677.771484375, Classification Loss: 1.3530399799346924\n",
      "Batch: 42, Total Loss = 16910.0078125 Contrastive Loss: 16908.25, Classification Loss: 1.7586778402328491\n",
      "Batch: 43, Total Loss = 16034.974609375 Contrastive Loss: 16034.2509765625, Classification Loss: 0.7238445281982422\n",
      "Batch: 44, Total Loss = 16533.10546875 Contrastive Loss: 16531.361328125, Classification Loss: 1.7444140911102295\n",
      "Batch: 45, Total Loss = 16575.501953125 Contrastive Loss: 16573.58203125, Classification Loss: 1.920172929763794\n",
      "Batch: 46, Total Loss = 14421.5498046875 Contrastive Loss: 14421.349609375, Classification Loss: 0.2004540115594864\n",
      "Batch: 47, Total Loss = 15347.470703125 Contrastive Loss: 15346.416015625, Classification Loss: 1.0544859170913696\n",
      "Batch: 48, Total Loss = 18121.9375 Contrastive Loss: 18120.421875, Classification Loss: 1.5155168771743774\n",
      "Batch: 49, Total Loss = 16681.38671875 Contrastive Loss: 16680.55859375, Classification Loss: 0.8274289965629578\n",
      "Batch: 50, Total Loss = 16450.45703125 Contrastive Loss: 16449.548828125, Classification Loss: 0.9077353477478027\n",
      "Batch: 51, Total Loss = 15494.7998046875 Contrastive Loss: 15493.810546875, Classification Loss: 0.9890052080154419\n",
      "Batch: 52, Total Loss = 15657.7548828125 Contrastive Loss: 15657.349609375, Classification Loss: 0.4048725366592407\n",
      "Batch: 53, Total Loss = 15843.041015625 Contrastive Loss: 15841.921875, Classification Loss: 1.1195709705352783\n",
      "Batch: 54, Total Loss = 15953.4091796875 Contrastive Loss: 15952.044921875, Classification Loss: 1.364516019821167\n",
      "Batch: 55, Total Loss = 15916.3095703125 Contrastive Loss: 15915.12109375, Classification Loss: 1.1881933212280273\n",
      "Batch: 56, Total Loss = 16532.14453125 Contrastive Loss: 16531.046875, Classification Loss: 1.0974230766296387\n",
      "Batch: 57, Total Loss = 16103.076171875 Contrastive Loss: 16102.2275390625, Classification Loss: 0.849104642868042\n",
      "Batch: 58, Total Loss = 16636.75390625 Contrastive Loss: 16636.109375, Classification Loss: 0.644985020160675\n",
      "Batch: 59, Total Loss = 16684.474609375 Contrastive Loss: 16683.306640625, Classification Loss: 1.1683027744293213\n",
      "Batch: 60, Total Loss = 16752.2109375 Contrastive Loss: 16751.27734375, Classification Loss: 0.9343340396881104\n",
      "Batch: 61, Total Loss = 16168.8330078125 Contrastive Loss: 16168.138671875, Classification Loss: 0.6947693824768066\n",
      "Batch: 62, Total Loss = 15516.4921875 Contrastive Loss: 15515.66015625, Classification Loss: 0.8324236273765564\n",
      "Batch: 63, Total Loss = 15788.9189453125 Contrastive Loss: 15788.3349609375, Classification Loss: 0.5841124057769775\n",
      "Batch: 64, Total Loss = 15936.3251953125 Contrastive Loss: 15935.7880859375, Classification Loss: 0.5368030071258545\n",
      "Batch: 65, Total Loss = 15718.712890625 Contrastive Loss: 15718.232421875, Classification Loss: 0.4800489544868469\n",
      "Batch: 66, Total Loss = 15873.7509765625 Contrastive Loss: 15873.0439453125, Classification Loss: 0.7073855996131897\n",
      "Batch: 67, Total Loss = 16072.4013671875 Contrastive Loss: 16071.69921875, Classification Loss: 0.7025787234306335\n",
      "Batch: 68, Total Loss = 15389.8857421875 Contrastive Loss: 15389.416015625, Classification Loss: 0.4695357084274292\n",
      "Batch: 69, Total Loss = 15867.98046875 Contrastive Loss: 15867.0576171875, Classification Loss: 0.922915518283844\n",
      "Batch: 70, Total Loss = 14954.412109375 Contrastive Loss: 14953.8984375, Classification Loss: 0.5141078233718872\n",
      "Batch: 71, Total Loss = 15632.400390625 Contrastive Loss: 15631.86328125, Classification Loss: 0.5367431640625\n",
      "Batch: 72, Total Loss = 15761.41796875 Contrastive Loss: 15760.8466796875, Classification Loss: 0.5713662505149841\n",
      "Batch: 73, Total Loss = 15889.705078125 Contrastive Loss: 15888.8330078125, Classification Loss: 0.8722750544548035\n",
      "Batch: 74, Total Loss = 15954.830078125 Contrastive Loss: 15954.15234375, Classification Loss: 0.6779116988182068\n",
      "Batch: 75, Total Loss = 16401.82421875 Contrastive Loss: 16401.125, Classification Loss: 0.6994379162788391\n",
      "Batch: 76, Total Loss = 15719.279296875 Contrastive Loss: 15718.796875, Classification Loss: 0.4829076826572418\n",
      "Batch: 77, Total Loss = 16008.7353515625 Contrastive Loss: 16007.697265625, Classification Loss: 1.0376518964767456\n",
      "Batch: 78, Total Loss = 16574.716796875 Contrastive Loss: 16573.88671875, Classification Loss: 0.8305050134658813\n",
      "Batch: 79, Total Loss = 15625.880859375 Contrastive Loss: 15624.92578125, Classification Loss: 0.9550066590309143\n",
      "Batch: 80, Total Loss = 18615.876953125 Contrastive Loss: 18614.5234375, Classification Loss: 1.3527148962020874\n",
      "Batch: 81, Total Loss = 17808.587890625 Contrastive Loss: 17807.21484375, Classification Loss: 1.3726340532302856\n",
      "Batch: 82, Total Loss = 15794.0517578125 Contrastive Loss: 15792.5107421875, Classification Loss: 1.540932297706604\n",
      "Batch: 83, Total Loss = 18318.025390625 Contrastive Loss: 18317.55859375, Classification Loss: 0.4672403633594513\n",
      "Batch: 84, Total Loss = 15654.19921875 Contrastive Loss: 15653.6708984375, Classification Loss: 0.5286874175071716\n",
      "Batch: 85, Total Loss = 16794.173828125 Contrastive Loss: 16793.265625, Classification Loss: 0.9084290862083435\n",
      "Batch: 86, Total Loss = 15731.0146484375 Contrastive Loss: 15730.560546875, Classification Loss: 0.45383763313293457\n",
      "Batch: 87, Total Loss = 19549.423828125 Contrastive Loss: 19546.90234375, Classification Loss: 2.5205702781677246\n",
      "Batch: 88, Total Loss = 12839.822265625 Contrastive Loss: 12838.7998046875, Classification Loss: 1.022710919380188\n",
      "Training loss: 16298.616\n",
      "Training contrastive loss: 16297.523\n",
      "Training classification loss: 1.093\n",
      "Validation loss: nan\n",
      "Validation superclass acc: 76.62 %\n",
      "\n",
      "Epoch 5\n",
      "Batch: 0, Total Loss = 15275.392578125 Contrastive Loss: 15274.6171875, Classification Loss: 0.7757923007011414\n",
      "Batch: 1, Total Loss = 15300.7392578125 Contrastive Loss: 15300.1669921875, Classification Loss: 0.5720908641815186\n",
      "Batch: 2, Total Loss = 16030.51953125 Contrastive Loss: 16028.4794921875, Classification Loss: 2.039588689804077\n",
      "Batch: 3, Total Loss = 15198.3583984375 Contrastive Loss: 15197.4326171875, Classification Loss: 0.9262670874595642\n",
      "Batch: 4, Total Loss = 16169.8095703125 Contrastive Loss: 16168.115234375, Classification Loss: 1.6939491033554077\n",
      "Batch: 5, Total Loss = 15369.9306640625 Contrastive Loss: 15369.0537109375, Classification Loss: 0.8769754767417908\n",
      "Batch: 6, Total Loss = 15652.046875 Contrastive Loss: 15650.7255859375, Classification Loss: 1.3211085796356201\n",
      "Batch: 7, Total Loss = 15065.90625 Contrastive Loss: 15064.5654296875, Classification Loss: 1.3412803411483765\n",
      "Batch: 8, Total Loss = 16506.21484375 Contrastive Loss: 16504.2265625, Classification Loss: 1.9873195886611938\n",
      "Batch: 9, Total Loss = 15975.2236328125 Contrastive Loss: 15974.439453125, Classification Loss: 0.7846676707267761\n",
      "Batch: 10, Total Loss = 15464.0732421875 Contrastive Loss: 15463.6259765625, Classification Loss: 0.44737857580184937\n",
      "Batch: 11, Total Loss = 17023.779296875 Contrastive Loss: 17022.212890625, Classification Loss: 1.5665769577026367\n",
      "Batch: 12, Total Loss = 16578.13671875 Contrastive Loss: 16576.22265625, Classification Loss: 1.9149188995361328\n",
      "Batch: 13, Total Loss = 15445.556640625 Contrastive Loss: 15443.9833984375, Classification Loss: 1.573489785194397\n",
      "Batch: 14, Total Loss = 16933.896484375 Contrastive Loss: 16932.8359375, Classification Loss: 1.0608470439910889\n",
      "Batch: 15, Total Loss = 16107.2333984375 Contrastive Loss: 16106.7236328125, Classification Loss: 0.5096981525421143\n",
      "Batch: 16, Total Loss = 15652.7734375 Contrastive Loss: 15651.146484375, Classification Loss: 1.6268641948699951\n",
      "Batch: 17, Total Loss = 15812.6396484375 Contrastive Loss: 15811.3271484375, Classification Loss: 1.3120263814926147\n",
      "Batch: 18, Total Loss = 15216.9697265625 Contrastive Loss: 15216.501953125, Classification Loss: 0.46756890416145325\n",
      "Batch: 19, Total Loss = 15739.6630859375 Contrastive Loss: 15738.3154296875, Classification Loss: 1.3475185632705688\n",
      "Batch: 20, Total Loss = 15249.3037109375 Contrastive Loss: 15248.17578125, Classification Loss: 1.1278901100158691\n",
      "Batch: 21, Total Loss = 15647.7568359375 Contrastive Loss: 15646.208984375, Classification Loss: 1.5478194952011108\n",
      "Batch: 22, Total Loss = 15086.86328125 Contrastive Loss: 15086.33984375, Classification Loss: 0.5233920216560364\n",
      "Batch: 23, Total Loss = 16129.8408203125 Contrastive Loss: 16129.087890625, Classification Loss: 0.7533179521560669\n",
      "Batch: 24, Total Loss = 15798.8671875 Contrastive Loss: 15797.9765625, Classification Loss: 0.8905884027481079\n",
      "Batch: 25, Total Loss = 15543.8818359375 Contrastive Loss: 15543.0908203125, Classification Loss: 0.7909147143363953\n",
      "Batch: 26, Total Loss = 17116.32421875 Contrastive Loss: 17114.427734375, Classification Loss: 1.897165298461914\n",
      "Batch: 27, Total Loss = 15182.546875 Contrastive Loss: 15181.9580078125, Classification Loss: 0.5889962315559387\n",
      "Batch: 28, Total Loss = 14681.748046875 Contrastive Loss: 14681.42578125, Classification Loss: 0.32177990674972534\n",
      "Batch: 29, Total Loss = 15715.076171875 Contrastive Loss: 15713.9931640625, Classification Loss: 1.0834554433822632\n",
      "Batch: 30, Total Loss = 19234.380859375 Contrastive Loss: 19232.962890625, Classification Loss: 1.4186720848083496\n",
      "Batch: 31, Total Loss = 16519.58984375 Contrastive Loss: 16519.095703125, Classification Loss: 0.4946810305118561\n",
      "Batch: 32, Total Loss = 15334.099609375 Contrastive Loss: 15331.8525390625, Classification Loss: 2.247159004211426\n",
      "Batch: 33, Total Loss = 16390.76953125 Contrastive Loss: 16390.1328125, Classification Loss: 0.636229932308197\n",
      "Batch: 34, Total Loss = 15800.3896484375 Contrastive Loss: 15799.0146484375, Classification Loss: 1.375415563583374\n",
      "Batch: 35, Total Loss = 15888.421875 Contrastive Loss: 15887.4931640625, Classification Loss: 0.9284587502479553\n",
      "Batch: 36, Total Loss = 15035.7353515625 Contrastive Loss: 15035.1015625, Classification Loss: 0.6334055066108704\n",
      "Batch: 37, Total Loss = 18860.21484375 Contrastive Loss: 18859.71875, Classification Loss: 0.49646276235580444\n",
      "Batch: 38, Total Loss = 16219.037109375 Contrastive Loss: 16217.4453125, Classification Loss: 1.592057228088379\n",
      "Batch: 39, Total Loss = 15791.6865234375 Contrastive Loss: 15790.359375, Classification Loss: 1.3274691104888916\n",
      "Batch: 40, Total Loss = 16195.0234375 Contrastive Loss: 16193.83984375, Classification Loss: 1.183552861213684\n",
      "Batch: 41, Total Loss = 17053.111328125 Contrastive Loss: 17051.19921875, Classification Loss: 1.9116661548614502\n",
      "Batch: 42, Total Loss = 15364.8740234375 Contrastive Loss: 15364.2109375, Classification Loss: 0.6626216769218445\n",
      "Batch: 43, Total Loss = 15799.560546875 Contrastive Loss: 15798.7646484375, Classification Loss: 0.7958260178565979\n",
      "Batch: 44, Total Loss = 15381.9892578125 Contrastive Loss: 15380.6943359375, Classification Loss: 1.2947150468826294\n",
      "Batch: 45, Total Loss = 16438.01171875 Contrastive Loss: 16437.7109375, Classification Loss: 0.3004745841026306\n",
      "Batch: 46, Total Loss = 15435.4541015625 Contrastive Loss: 15434.6806640625, Classification Loss: 0.7737754583358765\n",
      "Batch: 47, Total Loss = 15924.8916015625 Contrastive Loss: 15923.8310546875, Classification Loss: 1.0608474016189575\n",
      "Batch: 48, Total Loss = 15235.7724609375 Contrastive Loss: 15234.8916015625, Classification Loss: 0.8806710243225098\n",
      "Batch: 49, Total Loss = 16608.794921875 Contrastive Loss: 16607.2421875, Classification Loss: 1.5533289909362793\n",
      "Batch: 50, Total Loss = 16971.267578125 Contrastive Loss: 16969.263671875, Classification Loss: 2.004284143447876\n",
      "Batch: 51, Total Loss = 15249.1064453125 Contrastive Loss: 15248.2919921875, Classification Loss: 0.8146476149559021\n",
      "Batch: 52, Total Loss = 14657.8896484375 Contrastive Loss: 14657.4453125, Classification Loss: 0.4442746043205261\n",
      "Batch: 53, Total Loss = 15122.642578125 Contrastive Loss: 15122.05859375, Classification Loss: 0.5837844610214233\n",
      "Batch: 54, Total Loss = 16716.671875 Contrastive Loss: 16715.69140625, Classification Loss: 0.9802510142326355\n",
      "Batch: 55, Total Loss = 17014.20703125 Contrastive Loss: 17011.43359375, Classification Loss: 2.772986650466919\n",
      "Batch: 56, Total Loss = 15753.2421875 Contrastive Loss: 15751.7705078125, Classification Loss: 1.471312165260315\n",
      "Batch: 57, Total Loss = 15778.3203125 Contrastive Loss: 15777.677734375, Classification Loss: 0.642474889755249\n",
      "Batch: 58, Total Loss = 15604.8466796875 Contrastive Loss: 15603.73046875, Classification Loss: 1.116567850112915\n",
      "Batch: 59, Total Loss = 15055.1103515625 Contrastive Loss: 15054.3408203125, Classification Loss: 0.7698186635971069\n",
      "Batch: 60, Total Loss = 17344.572265625 Contrastive Loss: 17342.044921875, Classification Loss: 2.52693247795105\n",
      "Batch: 61, Total Loss = 14865.783203125 Contrastive Loss: 14865.3271484375, Classification Loss: 0.4558072090148926\n",
      "Batch: 62, Total Loss = 16161.87109375 Contrastive Loss: 16160.869140625, Classification Loss: 1.002431869506836\n",
      "Batch: 63, Total Loss = 15797.744140625 Contrastive Loss: 15796.810546875, Classification Loss: 0.9337064027786255\n",
      "Batch: 64, Total Loss = 14982.7861328125 Contrastive Loss: 14980.810546875, Classification Loss: 1.975918173789978\n",
      "Batch: 65, Total Loss = 16224.6376953125 Contrastive Loss: 16223.703125, Classification Loss: 0.9346386790275574\n",
      "Batch: 66, Total Loss = 19524.435546875 Contrastive Loss: 19523.32421875, Classification Loss: 1.1119349002838135\n",
      "Batch: 67, Total Loss = 16029.7509765625 Contrastive Loss: 16028.8056640625, Classification Loss: 0.9453120231628418\n",
      "Batch: 68, Total Loss = 16364.6513671875 Contrastive Loss: 16363.3359375, Classification Loss: 1.3152501583099365\n",
      "Batch: 69, Total Loss = 15064.0283203125 Contrastive Loss: 15063.068359375, Classification Loss: 0.9603713750839233\n",
      "Batch: 70, Total Loss = 16604.673828125 Contrastive Loss: 16604.08984375, Classification Loss: 0.5830578207969666\n",
      "Batch: 71, Total Loss = 17500.515625 Contrastive Loss: 17499.587890625, Classification Loss: 0.9281327128410339\n",
      "Batch: 72, Total Loss = 17928.74609375 Contrastive Loss: 17926.849609375, Classification Loss: 1.8966038227081299\n",
      "Batch: 73, Total Loss = 15889.3896484375 Contrastive Loss: 15888.48828125, Classification Loss: 0.9010201096534729\n",
      "Batch: 74, Total Loss = 15651.1328125 Contrastive Loss: 15650.1298828125, Classification Loss: 1.0024514198303223\n",
      "Batch: 75, Total Loss = 17539.3671875 Contrastive Loss: 17538.0234375, Classification Loss: 1.3445640802383423\n",
      "Batch: 76, Total Loss = 16054.009765625 Contrastive Loss: 16053.1796875, Classification Loss: 0.8296372890472412\n",
      "Batch: 77, Total Loss = 16036.275390625 Contrastive Loss: 16034.580078125, Classification Loss: 1.6956429481506348\n",
      "Batch: 78, Total Loss = 15386.365234375 Contrastive Loss: 15385.3173828125, Classification Loss: 1.047622799873352\n",
      "Batch: 79, Total Loss = 15271.1083984375 Contrastive Loss: 15270.4228515625, Classification Loss: 0.6859148144721985\n",
      "Batch: 80, Total Loss = 15320.337890625 Contrastive Loss: 15319.751953125, Classification Loss: 0.5856178998947144\n",
      "Batch: 81, Total Loss = 15995.8798828125 Contrastive Loss: 15994.9091796875, Classification Loss: 0.9706588983535767\n",
      "Batch: 82, Total Loss = 16159.2333984375 Contrastive Loss: 16157.8251953125, Classification Loss: 1.408528208732605\n",
      "Batch: 83, Total Loss = 15917.6884765625 Contrastive Loss: 15916.8671875, Classification Loss: 0.8213405609130859\n",
      "Batch: 84, Total Loss = 16247.8134765625 Contrastive Loss: 16246.44140625, Classification Loss: 1.3719946146011353\n",
      "Batch: 85, Total Loss = 16442.33984375 Contrastive Loss: 16440.330078125, Classification Loss: 2.0088565349578857\n",
      "Batch: 86, Total Loss = 15576.154296875 Contrastive Loss: 15575.23046875, Classification Loss: 0.9241690635681152\n",
      "Batch: 87, Total Loss = 16167.6611328125 Contrastive Loss: 16166.873046875, Classification Loss: 0.7885069847106934\n",
      "Batch: 88, Total Loss = 12570.09375 Contrastive Loss: 12568.76171875, Classification Loss: 1.3316205739974976\n",
      "Training loss: 16155.969\n",
      "Training contrastive loss: 16154.831\n",
      "Training classification loss: 1.138\n",
      "Validation loss: nan\n",
      "Validation superclass acc: 71.72 %\n",
      "\n",
      "Epoch 6\n",
      "Batch: 0, Total Loss = 16169.8046875 Contrastive Loss: 16169.0859375, Classification Loss: 0.7192157506942749\n",
      "Batch: 1, Total Loss = 16241.2509765625 Contrastive Loss: 16239.376953125, Classification Loss: 1.873749017715454\n",
      "Batch: 2, Total Loss = 15885.916015625 Contrastive Loss: 15884.01953125, Classification Loss: 1.8962308168411255\n",
      "Batch: 3, Total Loss = 14884.458984375 Contrastive Loss: 14884.2333984375, Classification Loss: 0.2252325862646103\n",
      "Batch: 4, Total Loss = 16082.7685546875 Contrastive Loss: 16082.0498046875, Classification Loss: 0.718455970287323\n",
      "Batch: 5, Total Loss = 14500.8203125 Contrastive Loss: 14499.83984375, Classification Loss: 0.9804540276527405\n",
      "Batch: 6, Total Loss = 16271.685546875 Contrastive Loss: 16270.6455078125, Classification Loss: 1.0404521226882935\n",
      "Batch: 7, Total Loss = 14760.6650390625 Contrastive Loss: 14759.41015625, Classification Loss: 1.2550969123840332\n",
      "Batch: 8, Total Loss = 15130.7646484375 Contrastive Loss: 15129.8212890625, Classification Loss: 0.9433721303939819\n",
      "Batch: 9, Total Loss = 14785.10546875 Contrastive Loss: 14784.75, Classification Loss: 0.35555338859558105\n",
      "Batch: 10, Total Loss = 17147.220703125 Contrastive Loss: 17146.384765625, Classification Loss: 0.8361607193946838\n",
      "Batch: 11, Total Loss = 17070.6484375 Contrastive Loss: 17070.240234375, Classification Loss: 0.40784764289855957\n",
      "Batch: 12, Total Loss = 14388.6630859375 Contrastive Loss: 14387.923828125, Classification Loss: 0.7391136288642883\n",
      "Batch: 13, Total Loss = 14599.939453125 Contrastive Loss: 14598.2177734375, Classification Loss: 1.721343755722046\n",
      "Batch: 14, Total Loss = 16076.6845703125 Contrastive Loss: 16073.5, Classification Loss: 3.1848978996276855\n",
      "Batch: 15, Total Loss = 16830.474609375 Contrastive Loss: 16826.962890625, Classification Loss: 3.5122783184051514\n",
      "Batch: 16, Total Loss = 16088.9453125 Contrastive Loss: 16086.6533203125, Classification Loss: 2.2915358543395996\n",
      "Batch: 17, Total Loss = 16314.685546875 Contrastive Loss: 16311.830078125, Classification Loss: 2.855372667312622\n",
      "Batch: 18, Total Loss = 16287.8544921875 Contrastive Loss: 16285.5009765625, Classification Loss: 2.3539459705352783\n",
      "Batch: 19, Total Loss = 15261.1982421875 Contrastive Loss: 15260.4287109375, Classification Loss: 0.7698639631271362\n",
      "Batch: 20, Total Loss = 15814.15625 Contrastive Loss: 15812.6533203125, Classification Loss: 1.5033326148986816\n",
      "Batch: 21, Total Loss = 16934.4921875 Contrastive Loss: 16933.12109375, Classification Loss: 1.3706237077713013\n",
      "Batch: 22, Total Loss = 15599.951171875 Contrastive Loss: 15598.873046875, Classification Loss: 1.0781909227371216\n",
      "Batch: 23, Total Loss = 15365.583984375 Contrastive Loss: 15363.587890625, Classification Loss: 1.9961920976638794\n",
      "Batch: 24, Total Loss = 16689.376953125 Contrastive Loss: 16688.7734375, Classification Loss: 0.6032025814056396\n",
      "Batch: 25, Total Loss = 15712.361328125 Contrastive Loss: 15711.548828125, Classification Loss: 0.8122252821922302\n",
      "Batch: 26, Total Loss = 15143.501953125 Contrastive Loss: 15143.37109375, Classification Loss: 0.1307155042886734\n",
      "Batch: 27, Total Loss = 15079.365234375 Contrastive Loss: 15078.3935546875, Classification Loss: 0.9719240069389343\n",
      "Batch: 28, Total Loss = 15315.10546875 Contrastive Loss: 15313.1474609375, Classification Loss: 1.958231806755066\n",
      "Batch: 29, Total Loss = 15757.3720703125 Contrastive Loss: 15756.5185546875, Classification Loss: 0.853676438331604\n",
      "Batch: 30, Total Loss = 15110.96484375 Contrastive Loss: 15109.798828125, Classification Loss: 1.1663533449172974\n",
      "Batch: 31, Total Loss = 15469.9833984375 Contrastive Loss: 15469.24609375, Classification Loss: 0.7376466989517212\n",
      "Batch: 32, Total Loss = 14468.833984375 Contrastive Loss: 14468.2880859375, Classification Loss: 0.5455186367034912\n",
      "Batch: 33, Total Loss = 17860.515625 Contrastive Loss: 17858.7890625, Classification Loss: 1.7267829179763794\n",
      "Batch: 34, Total Loss = 14296.46484375 Contrastive Loss: 14295.693359375, Classification Loss: 0.7714951634407043\n",
      "Batch: 35, Total Loss = 16421.97265625 Contrastive Loss: 16419.41796875, Classification Loss: 2.5537467002868652\n",
      "Batch: 36, Total Loss = 14813.9658203125 Contrastive Loss: 14812.5615234375, Classification Loss: 1.404450535774231\n",
      "Batch: 37, Total Loss = 14797.62109375 Contrastive Loss: 14796.6572265625, Classification Loss: 0.9641479849815369\n",
      "Batch: 38, Total Loss = 15883.255859375 Contrastive Loss: 15882.5771484375, Classification Loss: 0.6783222556114197\n",
      "Batch: 39, Total Loss = 15939.728515625 Contrastive Loss: 15937.57421875, Classification Loss: 2.154064416885376\n",
      "Batch: 40, Total Loss = 15494.1435546875 Contrastive Loss: 15492.87109375, Classification Loss: 1.2725682258605957\n",
      "Batch: 41, Total Loss = 16620.0703125 Contrastive Loss: 16618.28515625, Classification Loss: 1.7850711345672607\n",
      "Batch: 42, Total Loss = 15616.83203125 Contrastive Loss: 15616.1484375, Classification Loss: 0.6838619112968445\n",
      "Batch: 43, Total Loss = 18747.53515625 Contrastive Loss: 18744.87890625, Classification Loss: 2.655468463897705\n",
      "Batch: 44, Total Loss = 16245.3681640625 Contrastive Loss: 16244.533203125, Classification Loss: 0.8353749513626099\n",
      "Batch: 45, Total Loss = 16091.646484375 Contrastive Loss: 16091.029296875, Classification Loss: 0.6174135804176331\n",
      "Batch: 46, Total Loss = 15821.43359375 Contrastive Loss: 15820.5078125, Classification Loss: 0.9253358840942383\n",
      "Batch: 47, Total Loss = 15072.8720703125 Contrastive Loss: 15071.4345703125, Classification Loss: 1.437745451927185\n",
      "Batch: 48, Total Loss = 16480.900390625 Contrastive Loss: 16479.263671875, Classification Loss: 1.6358460187911987\n",
      "Batch: 49, Total Loss = 15336.478515625 Contrastive Loss: 15335.65625, Classification Loss: 0.8223934769630432\n",
      "Batch: 50, Total Loss = 15395.4755859375 Contrastive Loss: 15394.314453125, Classification Loss: 1.1609386205673218\n",
      "Batch: 51, Total Loss = 15245.7822265625 Contrastive Loss: 15244.1357421875, Classification Loss: 1.6467257738113403\n",
      "Batch: 52, Total Loss = 15581.94140625 Contrastive Loss: 15580.123046875, Classification Loss: 1.8180749416351318\n",
      "Batch: 53, Total Loss = 16342.765625 Contrastive Loss: 16340.2822265625, Classification Loss: 2.4837849140167236\n",
      "Batch: 54, Total Loss = 16196.73828125 Contrastive Loss: 16195.4873046875, Classification Loss: 1.2510898113250732\n",
      "Batch: 55, Total Loss = 15434.650390625 Contrastive Loss: 15433.0390625, Classification Loss: 1.6112637519836426\n",
      "Batch: 56, Total Loss = 14741.1923828125 Contrastive Loss: 14739.9306640625, Classification Loss: 1.2621514797210693\n",
      "Batch: 57, Total Loss = 17439.763671875 Contrastive Loss: 17436.767578125, Classification Loss: 2.9958744049072266\n",
      "Batch: 58, Total Loss = 16025.5556640625 Contrastive Loss: 16024.166015625, Classification Loss: 1.3892552852630615\n",
      "Batch: 59, Total Loss = 16815.48828125 Contrastive Loss: 16813.935546875, Classification Loss: 1.5527734756469727\n",
      "Batch: 60, Total Loss = 15583.9423828125 Contrastive Loss: 15581.3369140625, Classification Loss: 2.6053714752197266\n",
      "Batch: 61, Total Loss = 15457.2548828125 Contrastive Loss: 15453.60546875, Classification Loss: 3.6494181156158447\n",
      "Batch: 62, Total Loss = 17429.75390625 Contrastive Loss: 17425.626953125, Classification Loss: 4.126307487487793\n",
      "Batch: 63, Total Loss = 16836.640625 Contrastive Loss: 16832.5859375, Classification Loss: 4.0552520751953125\n",
      "Batch: 64, Total Loss = 16451.189453125 Contrastive Loss: 16449.625, Classification Loss: 1.5643151998519897\n",
      "Batch: 65, Total Loss = 15340.4814453125 Contrastive Loss: 15339.91015625, Classification Loss: 0.5715492963790894\n",
      "Batch: 66, Total Loss = 16673.68359375 Contrastive Loss: 16671.908203125, Classification Loss: 1.775310754776001\n",
      "Batch: 67, Total Loss = 16549.58203125 Contrastive Loss: 16545.396484375, Classification Loss: 4.186507701873779\n",
      "Batch: 68, Total Loss = 17330.837890625 Contrastive Loss: 17324.31640625, Classification Loss: 6.521424770355225\n",
      "Batch: 69, Total Loss = 18024.55859375 Contrastive Loss: 18021.748046875, Classification Loss: 2.8102493286132812\n",
      "Batch: 70, Total Loss = 16070.125 Contrastive Loss: 16068.86328125, Classification Loss: 1.2614232301712036\n",
      "Batch: 71, Total Loss = 15078.265625 Contrastive Loss: 15077.1611328125, Classification Loss: 1.1048836708068848\n",
      "Batch: 72, Total Loss = 16168.208984375 Contrastive Loss: 16165.82421875, Classification Loss: 2.3849198818206787\n",
      "Batch: 73, Total Loss = 15350.4892578125 Contrastive Loss: 15347.853515625, Classification Loss: 2.6355702877044678\n",
      "Batch: 74, Total Loss = 16042.236328125 Contrastive Loss: 16036.3212890625, Classification Loss: 5.914577007293701\n",
      "Batch: 75, Total Loss = 16229.9326171875 Contrastive Loss: 16225.677734375, Classification Loss: 4.254586696624756\n",
      "Batch: 76, Total Loss = 15691.7705078125 Contrastive Loss: 15689.5380859375, Classification Loss: 2.2326292991638184\n",
      "Batch: 77, Total Loss = 15048.642578125 Contrastive Loss: 15046.720703125, Classification Loss: 1.922132134437561\n",
      "Batch: 78, Total Loss = 15954.87890625 Contrastive Loss: 15952.6240234375, Classification Loss: 2.255126714706421\n",
      "Batch: 79, Total Loss = 15815.923828125 Contrastive Loss: 15815.1923828125, Classification Loss: 0.7316724061965942\n",
      "Batch: 80, Total Loss = 16352.94921875 Contrastive Loss: 16349.462890625, Classification Loss: 3.4867000579833984\n",
      "Batch: 81, Total Loss = 18059.1796875 Contrastive Loss: 18057.572265625, Classification Loss: 1.6065419912338257\n",
      "Batch: 82, Total Loss = 15412.1630859375 Contrastive Loss: 15410.048828125, Classification Loss: 2.113985300064087\n",
      "Batch: 83, Total Loss = 16914.228515625 Contrastive Loss: 16913.328125, Classification Loss: 0.9008201360702515\n",
      "Batch: 84, Total Loss = 16473.041015625 Contrastive Loss: 16471.01953125, Classification Loss: 2.0210912227630615\n",
      "Batch: 85, Total Loss = 15518.1494140625 Contrastive Loss: 15517.546875, Classification Loss: 0.6026371121406555\n",
      "Batch: 86, Total Loss = 15841.6611328125 Contrastive Loss: 15840.404296875, Classification Loss: 1.2566980123519897\n",
      "Batch: 87, Total Loss = 14959.6279296875 Contrastive Loss: 14958.6279296875, Classification Loss: 0.9995232224464417\n",
      "Batch: 88, Total Loss = 12715.8828125 Contrastive Loss: 12714.8564453125, Classification Loss: 1.0260438919067383\n",
      "Training loss: 16061.091\n",
      "Training contrastive loss: 16059.352\n",
      "Training classification loss: 1.740\n",
      "Validation loss: nan\n",
      "Validation superclass acc: 81.36 %\n",
      "\n",
      "Epoch 7\n",
      "Batch: 0, Total Loss = 15845.142578125 Contrastive Loss: 15843.68359375, Classification Loss: 1.4592760801315308\n",
      "Batch: 1, Total Loss = 15090.7890625 Contrastive Loss: 15089.9501953125, Classification Loss: 0.8384013175964355\n",
      "Batch: 2, Total Loss = 16256.17578125 Contrastive Loss: 16254.818359375, Classification Loss: 1.3573353290557861\n",
      "Batch: 3, Total Loss = 16276.8330078125 Contrastive Loss: 16276.2275390625, Classification Loss: 0.605096697807312\n",
      "Batch: 4, Total Loss = 15399.83203125 Contrastive Loss: 15397.7666015625, Classification Loss: 2.065091609954834\n",
      "Batch: 5, Total Loss = 15075.3876953125 Contrastive Loss: 15074.599609375, Classification Loss: 0.7876825928688049\n",
      "Batch: 6, Total Loss = 16456.390625 Contrastive Loss: 16454.291015625, Classification Loss: 2.0990183353424072\n",
      "Batch: 7, Total Loss = 15012.38671875 Contrastive Loss: 15011.8544921875, Classification Loss: 0.5318493843078613\n",
      "Batch: 8, Total Loss = 16824.765625 Contrastive Loss: 16824.01953125, Classification Loss: 0.7463040351867676\n",
      "Batch: 9, Total Loss = 15827.6123046875 Contrastive Loss: 15826.314453125, Classification Loss: 1.2974778413772583\n",
      "Batch: 10, Total Loss = 14978.8896484375 Contrastive Loss: 14977.93359375, Classification Loss: 0.9560578465461731\n",
      "Batch: 11, Total Loss = 16435.865234375 Contrastive Loss: 16434.330078125, Classification Loss: 1.5356731414794922\n",
      "Batch: 12, Total Loss = 17038.515625 Contrastive Loss: 17037.833984375, Classification Loss: 0.6815648078918457\n",
      "Batch: 13, Total Loss = 15070.421875 Contrastive Loss: 15068.75390625, Classification Loss: 1.6684108972549438\n",
      "Batch: 14, Total Loss = 16169.0185546875 Contrastive Loss: 16166.94140625, Classification Loss: 2.0776007175445557\n",
      "Batch: 15, Total Loss = 14551.685546875 Contrastive Loss: 14551.1123046875, Classification Loss: 0.5730853080749512\n",
      "Batch: 16, Total Loss = 14749.9150390625 Contrastive Loss: 14748.787109375, Classification Loss: 1.1275869607925415\n",
      "Batch: 17, Total Loss = 16867.015625 Contrastive Loss: 16866.568359375, Classification Loss: 0.4477275013923645\n",
      "Batch: 18, Total Loss = 14673.921875 Contrastive Loss: 14673.2734375, Classification Loss: 0.6485443711280823\n",
      "Batch: 19, Total Loss = 15618.9560546875 Contrastive Loss: 15617.3828125, Classification Loss: 1.5733259916305542\n",
      "Batch: 20, Total Loss = 15393.2626953125 Contrastive Loss: 15392.1435546875, Classification Loss: 1.118902325630188\n",
      "Batch: 21, Total Loss = 15467.029296875 Contrastive Loss: 15465.796875, Classification Loss: 1.2319592237472534\n",
      "Batch: 22, Total Loss = 17165.556640625 Contrastive Loss: 17163.76953125, Classification Loss: 1.786562442779541\n",
      "Batch: 23, Total Loss = 16809.701171875 Contrastive Loss: 16809.01953125, Classification Loss: 0.6817609071731567\n",
      "Batch: 24, Total Loss = 15618.330078125 Contrastive Loss: 15617.5537109375, Classification Loss: 0.7761065363883972\n",
      "Batch: 25, Total Loss = 15309.603515625 Contrastive Loss: 15307.8935546875, Classification Loss: 1.7099167108535767\n",
      "Batch: 26, Total Loss = 16340.1494140625 Contrastive Loss: 16338.408203125, Classification Loss: 1.7414309978485107\n",
      "Batch: 27, Total Loss = 15509.7197265625 Contrastive Loss: 15508.44140625, Classification Loss: 1.278775930404663\n",
      "Batch: 28, Total Loss = 16921.330078125 Contrastive Loss: 16920.287109375, Classification Loss: 1.0437722206115723\n",
      "Batch: 29, Total Loss = 15258.4189453125 Contrastive Loss: 15257.353515625, Classification Loss: 1.065261960029602\n",
      "Batch: 30, Total Loss = 15818.0166015625 Contrastive Loss: 15816.345703125, Classification Loss: 1.6704670190811157\n",
      "Batch: 31, Total Loss = 16109.78515625 Contrastive Loss: 16108.7919921875, Classification Loss: 0.9935221076011658\n",
      "Batch: 32, Total Loss = 15101.2607421875 Contrastive Loss: 15100.62890625, Classification Loss: 0.6321426033973694\n",
      "Batch: 33, Total Loss = 14961.0595703125 Contrastive Loss: 14960.4072265625, Classification Loss: 0.652472198009491\n",
      "Batch: 34, Total Loss = 19131.546875 Contrastive Loss: 19130.646484375, Classification Loss: 0.8996216058731079\n",
      "Batch: 35, Total Loss = 15134.455078125 Contrastive Loss: 15132.5908203125, Classification Loss: 1.8647254705429077\n",
      "Batch: 36, Total Loss = 15712.6513671875 Contrastive Loss: 15711.9375, Classification Loss: 0.7141266465187073\n",
      "Batch: 37, Total Loss = 15612.498046875 Contrastive Loss: 15611.0771484375, Classification Loss: 1.4213743209838867\n",
      "Batch: 38, Total Loss = 15175.1845703125 Contrastive Loss: 15174.2021484375, Classification Loss: 0.9824897050857544\n",
      "Batch: 39, Total Loss = 18066.5859375 Contrastive Loss: 18065.900390625, Classification Loss: 0.6850454211235046\n",
      "Batch: 40, Total Loss = 15765.1708984375 Contrastive Loss: 15763.0009765625, Classification Loss: 2.16955828666687\n",
      "Batch: 41, Total Loss = 15901.455078125 Contrastive Loss: 15901.1123046875, Classification Loss: 0.3423752188682556\n",
      "Batch: 42, Total Loss = 15660.701171875 Contrastive Loss: 15659.212890625, Classification Loss: 1.4878215789794922\n",
      "Batch: 43, Total Loss = 15436.0947265625 Contrastive Loss: 15435.3955078125, Classification Loss: 0.6991332769393921\n",
      "Batch: 44, Total Loss = 16039.7578125 Contrastive Loss: 16038.84765625, Classification Loss: 0.9097837805747986\n",
      "Batch: 45, Total Loss = 15400.0849609375 Contrastive Loss: 15398.916015625, Classification Loss: 1.1684826612472534\n",
      "Batch: 46, Total Loss = 15512.4892578125 Contrastive Loss: 15511.63671875, Classification Loss: 0.8525568842887878\n",
      "Batch: 47, Total Loss = 15510.92578125 Contrastive Loss: 15509.2001953125, Classification Loss: 1.7260565757751465\n",
      "Batch: 48, Total Loss = 15140.8291015625 Contrastive Loss: 15140.26953125, Classification Loss: 0.5599965453147888\n",
      "Batch: 49, Total Loss = 14789.7578125 Contrastive Loss: 14789.162109375, Classification Loss: 0.595491886138916\n",
      "Batch: 50, Total Loss = 15748.7314453125 Contrastive Loss: 15748.45703125, Classification Loss: 0.2739628851413727\n",
      "Batch: 51, Total Loss = 15532.423828125 Contrastive Loss: 15531.59375, Classification Loss: 0.8299708962440491\n",
      "Batch: 52, Total Loss = 16305.11328125 Contrastive Loss: 16303.8212890625, Classification Loss: 1.292240858078003\n",
      "Batch: 53, Total Loss = 15752.705078125 Contrastive Loss: 15752.1474609375, Classification Loss: 0.5574807524681091\n",
      "Batch: 54, Total Loss = 19007.1328125 Contrastive Loss: 19005.634765625, Classification Loss: 1.4981768131256104\n",
      "Batch: 55, Total Loss = 18061.4296875 Contrastive Loss: 18059.921875, Classification Loss: 1.5084302425384521\n",
      "Batch: 56, Total Loss = 16405.125 Contrastive Loss: 16404.20703125, Classification Loss: 0.9170083999633789\n",
      "Batch: 57, Total Loss = 15220.0224609375 Contrastive Loss: 15219.0400390625, Classification Loss: 0.9823309183120728\n",
      "Batch: 58, Total Loss = 15197.1005859375 Contrastive Loss: 15196.435546875, Classification Loss: 0.6651194095611572\n",
      "Batch: 59, Total Loss = 16384.1328125 Contrastive Loss: 16383.4912109375, Classification Loss: 0.641201376914978\n",
      "Batch: 60, Total Loss = 16234.52734375 Contrastive Loss: 16233.1474609375, Classification Loss: 1.3801075220108032\n",
      "Batch: 61, Total Loss = 15921.2890625 Contrastive Loss: 15920.22265625, Classification Loss: 1.0667604207992554\n",
      "Batch: 62, Total Loss = 16246.7626953125 Contrastive Loss: 16246.0078125, Classification Loss: 0.754660964012146\n",
      "Batch: 63, Total Loss = 15728.0322265625 Contrastive Loss: 15727.1904296875, Classification Loss: 0.841939389705658\n",
      "Batch: 64, Total Loss = 16431.255859375 Contrastive Loss: 16430.21875, Classification Loss: 1.0367577075958252\n",
      "Batch: 65, Total Loss = 16011.4873046875 Contrastive Loss: 16010.8505859375, Classification Loss: 0.6370769739151001\n",
      "Batch: 66, Total Loss = 15982.08203125 Contrastive Loss: 15981.0390625, Classification Loss: 1.0432207584381104\n",
      "Batch: 67, Total Loss = 16720.783203125 Contrastive Loss: 16720.404296875, Classification Loss: 0.37820613384246826\n",
      "Batch: 68, Total Loss = 16337.2890625 Contrastive Loss: 16335.6279296875, Classification Loss: 1.661577820777893\n",
      "Batch: 69, Total Loss = 17453.35546875 Contrastive Loss: 17452.5859375, Classification Loss: 0.7704677581787109\n",
      "Batch: 70, Total Loss = 15954.759765625 Contrastive Loss: 15953.9189453125, Classification Loss: 0.8407853841781616\n",
      "Batch: 71, Total Loss = 15808.2431640625 Contrastive Loss: 15807.28515625, Classification Loss: 0.9580756425857544\n",
      "Batch: 72, Total Loss = 16845.0390625 Contrastive Loss: 16843.880859375, Classification Loss: 1.1588550806045532\n",
      "Batch: 73, Total Loss = 15961.40234375 Contrastive Loss: 15961.017578125, Classification Loss: 0.385039746761322\n",
      "Batch: 74, Total Loss = 15474.92578125 Contrastive Loss: 15474.380859375, Classification Loss: 0.5444602370262146\n",
      "Batch: 75, Total Loss = 15692.6748046875 Contrastive Loss: 15691.36328125, Classification Loss: 1.3112051486968994\n",
      "Batch: 76, Total Loss = 15590.513671875 Contrastive Loss: 15589.5263671875, Classification Loss: 0.9869005680084229\n",
      "Batch: 77, Total Loss = 16206.990234375 Contrastive Loss: 16205.9375, Classification Loss: 1.0530658960342407\n",
      "Batch: 78, Total Loss = 18265.259765625 Contrastive Loss: 18263.27734375, Classification Loss: 1.982474684715271\n",
      "Batch: 79, Total Loss = 15524.9033203125 Contrastive Loss: 15524.2568359375, Classification Loss: 0.646751344203949\n",
      "Batch: 80, Total Loss = 15172.48046875 Contrastive Loss: 15171.2724609375, Classification Loss: 1.2078274488449097\n",
      "Batch: 81, Total Loss = 16140.041015625 Contrastive Loss: 16139.47265625, Classification Loss: 0.5686161518096924\n",
      "Batch: 82, Total Loss = 15032.1162109375 Contrastive Loss: 15031.361328125, Classification Loss: 0.7550951242446899\n",
      "Batch: 83, Total Loss = 15439.5908203125 Contrastive Loss: 15439.05859375, Classification Loss: 0.5324073433876038\n",
      "Batch: 84, Total Loss = 15449.560546875 Contrastive Loss: 15448.931640625, Classification Loss: 0.6293751001358032\n",
      "Batch: 85, Total Loss = 15425.1806640625 Contrastive Loss: 15424.5703125, Classification Loss: 0.6102125644683838\n",
      "Batch: 86, Total Loss = 15269.8671875 Contrastive Loss: 15269.4267578125, Classification Loss: 0.44090157747268677\n",
      "Batch: 87, Total Loss = 16252.701171875 Contrastive Loss: 16252.181640625, Classification Loss: 0.5193460583686829\n",
      "Batch: 88, Total Loss = 12112.6513671875 Contrastive Loss: 12112.2265625, Classification Loss: 0.4250907003879547\n",
      "Training loss: 16071.507\n",
      "Training contrastive loss: 16070.475\n",
      "Training classification loss: 1.032\n",
      "Validation loss: nan\n",
      "Validation superclass acc: 66.67 %\n",
      "\n",
      "Epoch 8\n",
      "Batch: 0, Total Loss = 14603.376953125 Contrastive Loss: 14602.7060546875, Classification Loss: 0.6707622408866882\n",
      "Batch: 1, Total Loss = 15845.80078125 Contrastive Loss: 15845.1669921875, Classification Loss: 0.6339824795722961\n",
      "Batch: 2, Total Loss = 15070.7900390625 Contrastive Loss: 15070.240234375, Classification Loss: 0.5494368672370911\n",
      "Batch: 3, Total Loss = 15260.8505859375 Contrastive Loss: 15259.8623046875, Classification Loss: 0.988568127155304\n",
      "Batch: 4, Total Loss = 16565.875 Contrastive Loss: 16564.935546875, Classification Loss: 0.9388558268547058\n",
      "Batch: 5, Total Loss = 19296.609375 Contrastive Loss: 19296.03515625, Classification Loss: 0.5736868381500244\n",
      "Batch: 6, Total Loss = 14734.9404296875 Contrastive Loss: 14733.9765625, Classification Loss: 0.963943600654602\n",
      "Batch: 7, Total Loss = 15196.6171875 Contrastive Loss: 15195.0771484375, Classification Loss: 1.5403797626495361\n",
      "Batch: 8, Total Loss = 15850.3154296875 Contrastive Loss: 15848.591796875, Classification Loss: 1.7235233783721924\n",
      "Batch: 9, Total Loss = 17403.744140625 Contrastive Loss: 17403.2109375, Classification Loss: 0.534082293510437\n",
      "Batch: 10, Total Loss = 16494.38671875 Contrastive Loss: 16493.72265625, Classification Loss: 0.6634553670883179\n",
      "Batch: 11, Total Loss = 15249.3828125 Contrastive Loss: 15247.0771484375, Classification Loss: 2.3053138256073\n",
      "Batch: 12, Total Loss = 16588.27734375 Contrastive Loss: 16587.4765625, Classification Loss: 0.8009076714515686\n",
      "Batch: 13, Total Loss = 16173.626953125 Contrastive Loss: 16172.4228515625, Classification Loss: 1.204448938369751\n",
      "Batch: 14, Total Loss = 16834.01171875 Contrastive Loss: 16831.30859375, Classification Loss: 2.7039215564727783\n",
      "Batch: 15, Total Loss = 15335.5888671875 Contrastive Loss: 15334.7578125, Classification Loss: 0.8312338590621948\n",
      "Batch: 16, Total Loss = 15121.7734375 Contrastive Loss: 15121.2509765625, Classification Loss: 0.5226669907569885\n",
      "Batch: 17, Total Loss = 15179.6220703125 Contrastive Loss: 15178.6650390625, Classification Loss: 0.957443118095398\n",
      "Batch: 18, Total Loss = 16255.181640625 Contrastive Loss: 16252.630859375, Classification Loss: 2.550466537475586\n",
      "Batch: 19, Total Loss = 16206.82421875 Contrastive Loss: 16202.68359375, Classification Loss: 4.14021635055542\n",
      "Batch: 20, Total Loss = 16049.234375 Contrastive Loss: 16047.1591796875, Classification Loss: 2.075441837310791\n",
      "Batch: 21, Total Loss = 16521.236328125 Contrastive Loss: 16520.109375, Classification Loss: 1.1270427703857422\n",
      "Batch: 22, Total Loss = 15242.9306640625 Contrastive Loss: 15242.396484375, Classification Loss: 0.5343100428581238\n",
      "Batch: 23, Total Loss = 15558.595703125 Contrastive Loss: 15556.369140625, Classification Loss: 2.2266151905059814\n",
      "Batch: 24, Total Loss = 16391.455078125 Contrastive Loss: 16390.974609375, Classification Loss: 0.4796386957168579\n",
      "Batch: 25, Total Loss = 15739.2939453125 Contrastive Loss: 15738.396484375, Classification Loss: 0.8977042436599731\n",
      "Batch: 26, Total Loss = 15035.47265625 Contrastive Loss: 15033.546875, Classification Loss: 1.9253416061401367\n",
      "Batch: 27, Total Loss = 15257.904296875 Contrastive Loss: 15256.900390625, Classification Loss: 1.0038509368896484\n",
      "Batch: 28, Total Loss = 14916.5849609375 Contrastive Loss: 14915.6201171875, Classification Loss: 0.9645870327949524\n",
      "Batch: 29, Total Loss = 16435.8203125 Contrastive Loss: 16432.818359375, Classification Loss: 3.0019731521606445\n",
      "Batch: 30, Total Loss = 17905.037109375 Contrastive Loss: 17904.111328125, Classification Loss: 0.9255629777908325\n",
      "Batch: 31, Total Loss = 16121.8125 Contrastive Loss: 16120.3544921875, Classification Loss: 1.4582189321517944\n",
      "Batch: 32, Total Loss = 16350.9833984375 Contrastive Loss: 16349.787109375, Classification Loss: 1.196364402770996\n",
      "Batch: 33, Total Loss = 15125.7578125 Contrastive Loss: 15125.146484375, Classification Loss: 0.6117022633552551\n",
      "Batch: 34, Total Loss = 15425.2294921875 Contrastive Loss: 15424.3330078125, Classification Loss: 0.896738588809967\n",
      "Batch: 35, Total Loss = 16502.97265625 Contrastive Loss: 16501.81640625, Classification Loss: 1.1571484804153442\n",
      "Batch: 36, Total Loss = 15443.6337890625 Contrastive Loss: 15443.0498046875, Classification Loss: 0.5837377309799194\n",
      "Batch: 37, Total Loss = 15131.578125 Contrastive Loss: 15130.517578125, Classification Loss: 1.0608946084976196\n",
      "Batch: 38, Total Loss = 15943.935546875 Contrastive Loss: 15943.3759765625, Classification Loss: 0.5597245097160339\n",
      "Batch: 39, Total Loss = 15201.5615234375 Contrastive Loss: 15200.9140625, Classification Loss: 0.6478822827339172\n",
      "Batch: 40, Total Loss = 18280.34375 Contrastive Loss: 18279.890625, Classification Loss: 0.45216259360313416\n",
      "Batch: 41, Total Loss = 17838.625 Contrastive Loss: 17837.6171875, Classification Loss: 1.0082378387451172\n",
      "Batch: 42, Total Loss = 15371.5341796875 Contrastive Loss: 15370.591796875, Classification Loss: 0.9425113201141357\n",
      "Batch: 43, Total Loss = 17009.76171875 Contrastive Loss: 17008.484375, Classification Loss: 1.2776410579681396\n",
      "Batch: 44, Total Loss = 15401.689453125 Contrastive Loss: 15400.712890625, Classification Loss: 0.976360559463501\n",
      "Batch: 45, Total Loss = 15631.55859375 Contrastive Loss: 15630.12890625, Classification Loss: 1.4293460845947266\n",
      "Batch: 46, Total Loss = 15241.48828125 Contrastive Loss: 15240.8046875, Classification Loss: 0.6837096810340881\n",
      "Batch: 47, Total Loss = 15776.59375 Contrastive Loss: 15775.8232421875, Classification Loss: 0.7703945636749268\n",
      "Batch: 48, Total Loss = 15638.423828125 Contrastive Loss: 15637.8056640625, Classification Loss: 0.6182317137718201\n",
      "Batch: 49, Total Loss = 15165.3271484375 Contrastive Loss: 15163.68359375, Classification Loss: 1.6435496807098389\n",
      "Batch: 50, Total Loss = 15231.5712890625 Contrastive Loss: 15230.6748046875, Classification Loss: 0.8964628577232361\n",
      "Batch: 51, Total Loss = 16540.84375 Contrastive Loss: 16539.7265625, Classification Loss: 1.1170330047607422\n",
      "Batch: 52, Total Loss = 15310.109375 Contrastive Loss: 15309.140625, Classification Loss: 0.9683765172958374\n",
      "Batch: 53, Total Loss = 17545.3125 Contrastive Loss: 17543.880859375, Classification Loss: 1.4324750900268555\n",
      "Batch: 54, Total Loss = 14602.810546875 Contrastive Loss: 14602.2001953125, Classification Loss: 0.6102861166000366\n",
      "Batch: 55, Total Loss = 16423.4765625 Contrastive Loss: 16422.296875, Classification Loss: 1.17919921875\n",
      "Batch: 56, Total Loss = 15023.458984375 Contrastive Loss: 15022.33984375, Classification Loss: 1.1191297769546509\n",
      "Batch: 57, Total Loss = 16909.185546875 Contrastive Loss: 16908.447265625, Classification Loss: 0.7386814951896667\n",
      "Batch: 58, Total Loss = 15084.564453125 Contrastive Loss: 15083.5576171875, Classification Loss: 1.0067245960235596\n",
      "Batch: 59, Total Loss = 14924.4375 Contrastive Loss: 14923.689453125, Classification Loss: 0.7483624219894409\n",
      "Batch: 60, Total Loss = 17068.705078125 Contrastive Loss: 17067.525390625, Classification Loss: 1.1790258884429932\n",
      "Batch: 61, Total Loss = 15134.0791015625 Contrastive Loss: 15132.8994140625, Classification Loss: 1.1795895099639893\n",
      "Batch: 62, Total Loss = 17903.537109375 Contrastive Loss: 17902.806640625, Classification Loss: 0.7295070886611938\n",
      "Batch: 63, Total Loss = 16888.658203125 Contrastive Loss: 16887.869140625, Classification Loss: 0.7888836860656738\n",
      "Batch: 64, Total Loss = 16160.029296875 Contrastive Loss: 16158.6728515625, Classification Loss: 1.3560667037963867\n",
      "Batch: 65, Total Loss = 14649.7236328125 Contrastive Loss: 14649.01171875, Classification Loss: 0.711928129196167\n",
      "Batch: 66, Total Loss = 14962.89453125 Contrastive Loss: 14961.1796875, Classification Loss: 1.7151663303375244\n",
      "Batch: 67, Total Loss = 18416.978515625 Contrastive Loss: 18415.63671875, Classification Loss: 1.3413965702056885\n",
      "Batch: 68, Total Loss = 14834.7265625 Contrastive Loss: 14834.2861328125, Classification Loss: 0.4407862424850464\n",
      "Batch: 69, Total Loss = 15774.5078125 Contrastive Loss: 15773.34375, Classification Loss: 1.1640993356704712\n",
      "Batch: 70, Total Loss = 15176.4248046875 Contrastive Loss: 15175.9189453125, Classification Loss: 0.5063221454620361\n",
      "Batch: 71, Total Loss = 15348.46875 Contrastive Loss: 15347.6953125, Classification Loss: 0.7737268209457397\n",
      "Batch: 72, Total Loss = 15249.2890625 Contrastive Loss: 15248.287109375, Classification Loss: 1.001974105834961\n",
      "Batch: 73, Total Loss = 15146.5732421875 Contrastive Loss: 15145.7421875, Classification Loss: 0.8314539790153503\n",
      "Batch: 74, Total Loss = 16543.970703125 Contrastive Loss: 16542.728515625, Classification Loss: 1.2415196895599365\n",
      "Batch: 75, Total Loss = 18508.171875 Contrastive Loss: 18506.38671875, Classification Loss: 1.7857871055603027\n",
      "Batch: 76, Total Loss = 15845.4755859375 Contrastive Loss: 15844.4892578125, Classification Loss: 0.986176609992981\n",
      "Batch: 77, Total Loss = 16088.41015625 Contrastive Loss: 16088.0615234375, Classification Loss: 0.3489709496498108\n",
      "Batch: 78, Total Loss = 15303.66796875 Contrastive Loss: 15302.7119140625, Classification Loss: 0.9561620950698853\n",
      "Batch: 79, Total Loss = 15897.380859375 Contrastive Loss: 15896.4931640625, Classification Loss: 0.8875183463096619\n",
      "Batch: 80, Total Loss = 15755.5166015625 Contrastive Loss: 15755.212890625, Classification Loss: 0.30398818850517273\n",
      "Batch: 81, Total Loss = 15705.3916015625 Contrastive Loss: 15704.544921875, Classification Loss: 0.8463514447212219\n",
      "Batch: 82, Total Loss = 16561.361328125 Contrastive Loss: 16561.03125, Classification Loss: 0.3298998177051544\n",
      "Batch: 83, Total Loss = 15171.111328125 Contrastive Loss: 15170.03125, Classification Loss: 1.0798118114471436\n",
      "Batch: 84, Total Loss = 16404.83203125 Contrastive Loss: 16404.3671875, Classification Loss: 0.4643845558166504\n",
      "Batch: 85, Total Loss = 15376.9765625 Contrastive Loss: 15376.3515625, Classification Loss: 0.6246504187583923\n",
      "Batch: 86, Total Loss = 17584.6328125 Contrastive Loss: 17583.90625, Classification Loss: 0.7257252931594849\n",
      "Batch: 87, Total Loss = 16547.66015625 Contrastive Loss: 16547.04296875, Classification Loss: 0.6177302598953247\n",
      "Batch: 88, Total Loss = 12394.54296875 Contrastive Loss: 12392.546875, Classification Loss: 1.996129035949707\n",
      "Training loss: 16101.403\n",
      "Training contrastive loss: 16100.316\n",
      "Training classification loss: 1.087\n",
      "Validation loss: nan\n",
      "Validation superclass acc: 84.68 %\n",
      "\n",
      "Epoch 9\n",
      "Batch: 0, Total Loss = 15081.142578125 Contrastive Loss: 15080.349609375, Classification Loss: 0.7927703857421875\n",
      "Batch: 1, Total Loss = 14853.9140625 Contrastive Loss: 14853.5556640625, Classification Loss: 0.35843008756637573\n",
      "Batch: 2, Total Loss = 15286.904296875 Contrastive Loss: 15285.75, Classification Loss: 1.1541364192962646\n",
      "Batch: 3, Total Loss = 14659.2607421875 Contrastive Loss: 14658.888671875, Classification Loss: 0.3716205358505249\n",
      "Batch: 4, Total Loss = 15521.5322265625 Contrastive Loss: 15520.4736328125, Classification Loss: 1.0583796501159668\n",
      "Batch: 5, Total Loss = 15973.671875 Contrastive Loss: 15972.29296875, Classification Loss: 1.3784656524658203\n",
      "Batch: 6, Total Loss = 15112.8212890625 Contrastive Loss: 15112.255859375, Classification Loss: 0.565631091594696\n",
      "Batch: 7, Total Loss = 14509.4296875 Contrastive Loss: 14509.3232421875, Classification Loss: 0.106932133436203\n",
      "Batch: 8, Total Loss = 15517.66796875 Contrastive Loss: 15517.1572265625, Classification Loss: 0.5103154182434082\n",
      "Batch: 9, Total Loss = 16774.509765625 Contrastive Loss: 16773.298828125, Classification Loss: 1.2108938694000244\n",
      "Batch: 10, Total Loss = 14122.939453125 Contrastive Loss: 14121.017578125, Classification Loss: 1.922133207321167\n",
      "Batch: 11, Total Loss = 16864.859375 Contrastive Loss: 16862.390625, Classification Loss: 2.468701124191284\n",
      "Batch: 12, Total Loss = 14615.1767578125 Contrastive Loss: 14613.7041015625, Classification Loss: 1.4723353385925293\n",
      "Batch: 13, Total Loss = 15888.443359375 Contrastive Loss: 15887.291015625, Classification Loss: 1.1521083116531372\n",
      "Batch: 14, Total Loss = 14472.8837890625 Contrastive Loss: 14472.435546875, Classification Loss: 0.4479893147945404\n",
      "Batch: 15, Total Loss = 16470.740234375 Contrastive Loss: 16469.015625, Classification Loss: 1.7238969802856445\n",
      "Batch: 16, Total Loss = 15749.7236328125 Contrastive Loss: 15748.6123046875, Classification Loss: 1.1111384630203247\n",
      "Batch: 17, Total Loss = 14804.2626953125 Contrastive Loss: 14803.40234375, Classification Loss: 0.8606767654418945\n",
      "Batch: 18, Total Loss = 15227.4677734375 Contrastive Loss: 15226.068359375, Classification Loss: 1.3996772766113281\n",
      "Batch: 19, Total Loss = 15401.015625 Contrastive Loss: 15399.5634765625, Classification Loss: 1.452435851097107\n",
      "Batch: 20, Total Loss = 15928.8857421875 Contrastive Loss: 15927.619140625, Classification Loss: 1.2669763565063477\n",
      "Batch: 21, Total Loss = 14726.6220703125 Contrastive Loss: 14725.923828125, Classification Loss: 0.6979529857635498\n",
      "Batch: 22, Total Loss = 15179.8896484375 Contrastive Loss: 15179.33984375, Classification Loss: 0.5494428873062134\n",
      "Batch: 23, Total Loss = 14435.42578125 Contrastive Loss: 14435.1865234375, Classification Loss: 0.23893673717975616\n",
      "Batch: 24, Total Loss = 14737.771484375 Contrastive Loss: 14736.3037109375, Classification Loss: 1.4682114124298096\n",
      "Batch: 25, Total Loss = 15109.244140625 Contrastive Loss: 15107.83984375, Classification Loss: 1.4044384956359863\n",
      "Batch: 26, Total Loss = 15025.228515625 Contrastive Loss: 15024.4658203125, Classification Loss: 0.7623572945594788\n",
      "Batch: 27, Total Loss = 14789.38671875 Contrastive Loss: 14788.9443359375, Classification Loss: 0.44213804602622986\n",
      "Batch: 28, Total Loss = 18019.576171875 Contrastive Loss: 18018.65234375, Classification Loss: 0.9231934547424316\n",
      "Batch: 29, Total Loss = 15519.953125 Contrastive Loss: 15519.1455078125, Classification Loss: 0.8073543310165405\n",
      "Batch: 30, Total Loss = 15311.0703125 Contrastive Loss: 15308.9287109375, Classification Loss: 2.1414268016815186\n",
      "Batch: 31, Total Loss = 15562.115234375 Contrastive Loss: 15561.1787109375, Classification Loss: 0.936104416847229\n",
      "Batch: 32, Total Loss = 15787.5078125 Contrastive Loss: 15786.65625, Classification Loss: 0.8511402010917664\n",
      "Batch: 33, Total Loss = 15725.9462890625 Contrastive Loss: 15724.951171875, Classification Loss: 0.9947935938835144\n",
      "Batch: 34, Total Loss = 14668.068359375 Contrastive Loss: 14667.44921875, Classification Loss: 0.6194034218788147\n",
      "Batch: 35, Total Loss = 15066.64453125 Contrastive Loss: 15066.34375, Classification Loss: 0.3008691370487213\n",
      "Batch: 36, Total Loss = 14212.1787109375 Contrastive Loss: 14211.4453125, Classification Loss: 0.7330120205879211\n",
      "Batch: 37, Total Loss = 15199.8369140625 Contrastive Loss: 15198.7373046875, Classification Loss: 1.0993890762329102\n",
      "Batch: 38, Total Loss = 16457.81640625 Contrastive Loss: 16456.6796875, Classification Loss: 1.136328935623169\n",
      "Batch: 39, Total Loss = 14700.9638671875 Contrastive Loss: 14700.478515625, Classification Loss: 0.4853900372982025\n",
      "Batch: 40, Total Loss = 15954.0234375 Contrastive Loss: 15952.951171875, Classification Loss: 1.0721641778945923\n",
      "Batch: 41, Total Loss = 15740.931640625 Contrastive Loss: 15739.990234375, Classification Loss: 0.9409894347190857\n",
      "Batch: 42, Total Loss = 18317.166015625 Contrastive Loss: 18314.87109375, Classification Loss: 2.294949769973755\n",
      "Batch: 43, Total Loss = 14276.130859375 Contrastive Loss: 14275.9560546875, Classification Loss: 0.17525061964988708\n",
      "Batch: 44, Total Loss = 15215.515625 Contrastive Loss: 15214.3046875, Classification Loss: 1.2109156847000122\n",
      "Batch: 45, Total Loss = 14943.9267578125 Contrastive Loss: 14942.24609375, Classification Loss: 1.6808847188949585\n",
      "Batch: 46, Total Loss = 15571.0966796875 Contrastive Loss: 15570.0947265625, Classification Loss: 1.0022287368774414\n",
      "Batch: 47, Total Loss = 16450.611328125 Contrastive Loss: 16449.513671875, Classification Loss: 1.0974235534667969\n",
      "Batch: 48, Total Loss = 15723.259765625 Contrastive Loss: 15722.5849609375, Classification Loss: 0.674410879611969\n",
      "Batch: 49, Total Loss = 15940.830078125 Contrastive Loss: 15940.224609375, Classification Loss: 0.6056392192840576\n",
      "Batch: 50, Total Loss = 16988.21484375 Contrastive Loss: 16986.375, Classification Loss: 1.839680790901184\n",
      "Batch: 51, Total Loss = 16163.904296875 Contrastive Loss: 16162.28515625, Classification Loss: 1.618924617767334\n",
      "Batch: 52, Total Loss = 15398.5029296875 Contrastive Loss: 15397.95703125, Classification Loss: 0.5455389618873596\n",
      "Batch: 53, Total Loss = 15295.3369140625 Contrastive Loss: 15294.5283203125, Classification Loss: 0.80861496925354\n",
      "Batch: 54, Total Loss = 15291.9853515625 Contrastive Loss: 15291.1796875, Classification Loss: 0.805988073348999\n",
      "Batch: 55, Total Loss = 15066.009765625 Contrastive Loss: 15064.23046875, Classification Loss: 1.7789489030838013\n",
      "Batch: 56, Total Loss = 14757.591796875 Contrastive Loss: 14757.1044921875, Classification Loss: 0.4877869486808777\n",
      "Batch: 57, Total Loss = 15948.4482421875 Contrastive Loss: 15947.18359375, Classification Loss: 1.2648413181304932\n",
      "Batch: 58, Total Loss = 15748.8994140625 Contrastive Loss: 15747.7802734375, Classification Loss: 1.1191575527191162\n",
      "Batch: 59, Total Loss = 14701.1962890625 Contrastive Loss: 14700.447265625, Classification Loss: 0.7492496967315674\n",
      "Batch: 60, Total Loss = 14851.3857421875 Contrastive Loss: 14850.75, Classification Loss: 0.6354728937149048\n",
      "Batch: 61, Total Loss = 16775.05859375 Contrastive Loss: 16774.484375, Classification Loss: 0.5733019709587097\n",
      "Batch: 62, Total Loss = 14635.93359375 Contrastive Loss: 14635.6474609375, Classification Loss: 0.2864559590816498\n",
      "Batch: 63, Total Loss = 18469.12890625 Contrastive Loss: 18468.41015625, Classification Loss: 0.7177846431732178\n",
      "Batch: 64, Total Loss = 14661.744140625 Contrastive Loss: 14660.5986328125, Classification Loss: 1.145953893661499\n",
      "Batch: 65, Total Loss = 15238.5048828125 Contrastive Loss: 15237.953125, Classification Loss: 0.5521760582923889\n",
      "Batch: 66, Total Loss = 14615.701171875 Contrastive Loss: 14615.0869140625, Classification Loss: 0.6147062182426453\n",
      "Batch: 67, Total Loss = 14776.5126953125 Contrastive Loss: 14776.150390625, Classification Loss: 0.3627170920372009\n",
      "Batch: 68, Total Loss = 14681.2861328125 Contrastive Loss: 14680.5830078125, Classification Loss: 0.7035681009292603\n",
      "Batch: 69, Total Loss = 14695.775390625 Contrastive Loss: 14695.3408203125, Classification Loss: 0.43433961272239685\n",
      "Batch: 70, Total Loss = 18871.03515625 Contrastive Loss: 18869.44140625, Classification Loss: 1.5927985906600952\n",
      "Batch: 71, Total Loss = 14684.361328125 Contrastive Loss: 14683.77734375, Classification Loss: 0.5836291313171387\n",
      "Batch: 72, Total Loss = 15611.4521484375 Contrastive Loss: 15610.8828125, Classification Loss: 0.5693520307540894\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\VEDANT~1\\AppData\\Local\\Temp/ipykernel_11968/2641270353.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Epoch {epoch+1}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VEDANT~1\\AppData\\Local\\Temp/ipykernel_11968/3797028760.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;31m# print(loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;31m# print(loss.size())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda3\\envs\\CompVisEnv\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda3\\envs\\CompVisEnv\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    trainer.train_epoch()\n",
    "    trainer.validate_epoch()\n",
    "    print('')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: nan\n",
      "Validation superclass acc: 85.94 %\n"
     ]
    }
   ],
   "source": [
    "trainer.validate_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=1,\n",
    "                         shuffle=False)\n",
    "\n",
    "# Init model and trainer\n",
    "device = 'cpu'\n",
    "model = MultiTailModel(model_name=\"resnet18\", target=\"superclass\").to(device)#CNN().to(device)\n",
    "criterion1 = SCL_fn #nn.CrossEntropyLoss()\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.5, momentum=0.9)\n",
    "trainer = Trainer(model, criterion1, criterion2, optimizer, train_loader, val_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'zero_grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\VEDANT~1\\AppData\\Local\\Temp/ipykernel_11968/2641270353.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Epoch {epoch+1}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VEDANT~1\\AppData\\Local\\Temp/ipykernel_11968/3796761310.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuper_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0msuper_class_feature_vect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuper_class_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;31m# super_outputs, class_logits = self.model(torch.unsqueeze(inputs))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'zero_grad'"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    trainer.train_epoch()\n",
    "    trainer.validate_epoch()\n",
    "    print('')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to C:\\Users\\Vedant Gannu/.cache\\torch\\hub\\checkpoints\\inception_v3_google-0cc3c7bd.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a5dc2185004247a268e8afae3e314a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/104M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Inception3(\n",
       "  (Conv2d_1a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_2a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_2b_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Conv2d_3b_1x1): BasicConv2d(\n",
       "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_4a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Mixed_5b): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_5c): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_5d): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6a): InceptionB(\n",
       "    (branch3x3): BasicConv2d(\n",
       "      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6b): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6c): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6d): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6e): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (AuxLogits): InceptionAux(\n",
       "    (conv0): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv1): BasicConv2d(\n",
       "      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  )\n",
       "  (Mixed_7a): InceptionD(\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_4): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_7b): InceptionE(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_7c): InceptionE(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.models.inception_v3(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to C:\\Users\\Vedant Gannu/.cache\\torch\\hub\\checkpoints\\densenet121-a639ec97.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877f9ce1349142269b21325648d082e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/30.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DenseNet(\n",
       "  (features): Sequential(\n",
       "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu0): ReLU(inplace=True)\n",
       "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (denseblock1): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition1): _Transition(\n",
       "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock2): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition2): _Transition(\n",
       "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock3): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer17): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer18): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer19): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer20): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer21): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer22): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer23): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer24): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition3): _Transition(\n",
       "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock4): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.models.densenet121(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
